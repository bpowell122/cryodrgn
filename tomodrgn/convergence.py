"""
Functions to aid estimation of model training convergence.
"""
import glob
import os
from string import ascii_uppercase
from typing import Literal

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt, image
from scipy import stats
from scipy.ndimage import maximum_filter, gaussian_filter
from scipy.spatial import distance_matrix

from tomodrgn import utils, analysis, mrc, models

log = utils.log


def fsc_referencevol_to_manyvols(reference_vol: str,
                                 vol_paths: list[str],
                                 fsc_mask: Literal['sphere', 'tight', 'soft', 'none'] | str | None,
                                 include_dc: bool = False) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Calculate the Fourier Shell Correlation (FSC) between one reference volume and many query volumes.

    :param reference_vol: path to the reference volume on disk
    :param vol_paths: list of paths to the query volumes on disk
    :param fsc_mask: type of real space mask to apply to each volume, determined using the reference volume.
            `Sphere` is a binary spherical mask inscribed within the volume cube.
            `Tight` is a binary mask determined at the 99.99th percentile of the volume.
            `Soft` is a soft (non-binary) mask created by dilating the tight mask 3 pixels and applying a further 10 pixel falling cosine edge.
            `None` or `'none'` (note the string) both result in no mask being applied to the volume.
    :param include_dc: Whether to return the full array of frequencies and FSCs, or truncate the DC component prior to returning the array.
    :return: array of resolution values in units 1/px and shape (boxsize // 2);
            array of FSC curves with shape (len(vol_paths), boxsize // 2);
            array of FSC metrics with shape (3, len(vol_paths)): resolution crossing FSC 0.143, resolution crossing FSC 0.5, integral under FSC before 0.143 crossing.
    """
    fsc_resolutions_point143 = []
    fsc_resolutions_point5 = []
    fsc_integrals = []
    fscs = []
    resolution = []
    for vol_path in vol_paths:
        print(f'Processing volume: {vol_path}')
        resolution, fsc = utils.calc_fsc(vol1=reference_vol,
                                         vol2=vol_path,
                                         mask=fsc_mask)
        if not include_dc:
            resolution = resolution[1:]
            fsc = fsc[1:]
        fscs.append(fsc)

        # find the resolution at which FSC crosses 0.5 correlation
        if np.all(fsc >= 0.5):
            threshold_resolution = float(resolution[-1])
            fsc_resolutions_point5.append(threshold_resolution)
        else:
            threshold_resolution = float(resolution[np.argmax(fsc < 0.5)])
            fsc_resolutions_point5.append(threshold_resolution)

        # find the resolution at which FSC crosses 0.143 correlation
        if np.all(fsc >= 0.143):
            threshold_resolution = float(resolution[-1])
            threshold_index = resolution.shape[0]
            fsc_resolutions_point143.append(threshold_resolution)
        else:
            threshold_resolution = float(resolution[np.argmax(fsc < 0.143)])
            threshold_index = np.argmax(fsc < 0.143)
            fsc_resolutions_point143.append(threshold_resolution)

        # calculate the integral of correlation against resolution
        fsc_integral = np.trapz(fsc[:threshold_index], resolution[:threshold_index])
        fsc_integrals.append(fsc_integral)

    fscs = np.asarray(fscs)
    fsc_metrics = np.array([fsc_resolutions_point143, fsc_resolutions_point5, fsc_integrals])

    return resolution, fscs, fsc_metrics


def plot_loss(runlog: str,
              outdir: str) -> None:
    """
    Plot the total loss, reconstruction loss, and KLD divergence per epoch.

    :param runlog: the run.log auto-generated by tomodrgn train_vae
    :param outdir: path to base directory to save outputs
    :return: None
    """

    _ = analysis.plot_losses(runlog)
    outpath = f'{outdir}/plots/00_total_loss.png'
    plt.savefig(outpath, dpi=300)
    log(f'Saved total loss plot to {outdir}/plots/00_total_loss.png')
    plt.close()


def plot_latent_pca(workdir: str,
                    outdir: str,
                    epochs: np.ndarray) -> None:
    """
    Calculates, saves, and plots PC1 vs PC2 for all particles' latent embeddings at selected epochs

    :param workdir: path to directory containing tomodrgn training results
    :param outdir: path to base directory to save outputs
    :param epochs: array of epochs to analyze
    :return: None
    """
    # run PCA on each epoch's latent embeddings
    epoch_pcs = []
    for epoch in epochs:
        log(f'Now calculating PCA for epoch {epoch}')
        z = utils.load_pkl(workdir + f'/z.{epoch}.train.pkl')
        pc, _ = analysis.run_pca(z)
        epoch_pcs.append(pc)
        utils.save_pkl(data=pc, out_pkl=f'{outdir}/pcs/pc.{epoch}.pkl')

    # plot pc1 vs pc2 for each epoch as a hexbin
    epoch_pcs = np.array(epoch_pcs)
    n_cols = int(np.ceil(len(epochs) ** 0.5))
    n_rows = int(np.ceil(len(epochs) / n_cols))
    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(3 * n_cols, 3 * n_rows), sharex='all', sharey='all')

    for (i, ax) in enumerate(axes.ravel()):
        try:
            # there is an epoch to plot on this axis
            pc = epoch_pcs[i]
            toplot = ax.hexbin(pc[:, 0],
                               pc[:, 1],
                               bins='log',
                               mincnt=1,
                               extent=(np.min(epoch_pcs[:, :, 0]),
                                       np.max(epoch_pcs[:, :, 0]),
                                       np.min(epoch_pcs[:, :, 1]),
                                       np.max(epoch_pcs[:, :, 1])))
            ax.set_title(f'epoch {epochs[i]}')
            if i % n_cols == 0:
                # set y axis label only for the left column of subplots
                ax.set_ylabel('l-PC2')
            if i >= n_cols * (n_rows - 1):
                # set the x axis label only for the bottom row of subplots
                ax.set_xlabel('l-PC1')
            if i == len(epochs) - 1:
                # add a colorbar of particle density
                fig.subplots_adjust(left=0.15, right=0.8, top=0.85, bottom=0.15)
                cbar_ax = fig.add_axes((0.82, 0.15, 0.02, 0.7))
                cbar = fig.colorbar(toplot, cax=cbar_ax)
                cbar.ax.set_ylabel('particle density', rotation=90)
        except IndexError:
            # this is an extra axis for which we do not have data to plot, hide it
            ax.axis('off')

    plt.savefig(f'{outdir}/plots/01_encoder_pcs.png', dpi=300)
    log(f'Saved PCA plots to {outdir}/plots/01_encoder_pcs.png', )
    plt.close()


def plot_latent_umap(workdir: str,
                     outdir: str,
                     epochs: np.ndarray,
                     nptcls: int,
                     subset: int | None = 50000,
                     random_seed: int | np.random.RandomState | None = 42,
                     random_state: int | None = 42) -> None:
    """
    Calculates, plots, and saves UMAP embeddings of subset of particles' selected epochs' latent encodings

    :param workdir: path to directory containing tomodrgn training results
    :param outdir: path to base directory to save outputs
    :param epochs: array of epochs for which to calculate UMAPs
    :param nptcls: number of particles in dataset
    :param subset: size of subset on which to calculate umap, None means all
    :param random_seed: seed for random selection of subset particles with numpy
    :param random_state: random state seed used by UMAP for reproducibility at slight cost of performance (None means faster but non-reproducible)
    :return: None
    """
    # set seeds
    np.random.seed(random_seed)

    # identify the subset of particles for which to calculate UMAP embeddings
    if subset is None:
        log('Using full particle stack for UMAP')
        ind_subset = np.arange(nptcls)
    else:
        nptcls_subset = min(nptcls, subset)
        log(f'Randomly selecting {nptcls_subset} particle subset on which to run UMAP (with random seed {random_seed})')
        ind_subset = np.sort(np.random.choice(a=nptcls, size=nptcls_subset, replace=False))
    utils.save_pkl(data=ind_subset, out_pkl=f'{outdir}/umaps/ind_subset.pkl')

    # calculate UMAP embeddings
    epoch_umaps = []
    for epoch in epochs:
        log(f'Now calculating UMAP for epoch {epoch}')
        z = utils.load_pkl(f'{workdir}/z.{epoch}.train.pkl')
        z = z[ind_subset]
        umap_emb, _ = analysis.run_umap(z=z, random_state=random_state, n_jobs=1)
        epoch_umaps.append(umap_emb)
        utils.save_pkl(data=umap_emb,
                       out_pkl=f'{outdir}/umaps/umap.{epoch}.pkl')

    # plot umap1 vs umap2 for each epoch as a hexbin
    epoch_umaps = np.array(epoch_umaps)
    n_cols = int(np.ceil(len(epochs) ** 0.5))
    n_rows = int(np.ceil(len(epochs) / n_cols))
    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(3 * n_cols, 3 * n_rows), sharex='all', sharey='all')

    for (i, ax) in enumerate(axes.ravel()):
        try:
            # there is an epoch to plot on this axis
            umap_emb = epoch_umaps[i]
            toplot = ax.hexbin(umap_emb[:, 0],
                               umap_emb[:, 1],
                               bins='log',
                               mincnt=1,
                               extent=(np.min(epoch_umaps[:, :, 0]),
                                       np.max(epoch_umaps[:, :, 0]),
                                       np.min(epoch_umaps[:, :, 1]),
                                       np.max(epoch_umaps[:, :, 1])))
            ax.set_title(f'epoch {epochs[i]}')
            if i % n_cols == 0:
                # set y axis label only for the left column of subplots
                ax.set_ylabel('l-UMAP2')
            if i >= n_cols * (n_rows - 1):
                # set the x axis label only for the bottom row of subplots
                ax.set_xlabel('l-UMAP1')
            if i == len(epochs) - 1:
                # add a colorbar of particle density
                fig.subplots_adjust(left=0.15, right=0.8, top=0.85, bottom=0.15)
                cbar_ax = fig.add_axes((0.82, 0.15, 0.02, 0.7))
                cbar = fig.colorbar(toplot, cax=cbar_ax)
                cbar.ax.set_ylabel('particle density', rotation=90)
        except IndexError:
            # this is an extra axis for which we do not have data to plot, hide it
            ax.axis('off')

    plt.savefig(f'{outdir}/plots/02_encoder_umaps.png', dpi=300)
    log(f'Saved UMAP distribution plot to {outdir}/plots/02_encoder_umaps.png')
    plt.close()


def encoder_latent_shifts(workdir: str,
                          outdir: str,
                          final_epoch: int) -> None:
    """
    Calculates and plots various metrics characterizing the per-particle latent vectors between successive epochs ranging between 0 and `final_epoch`.

    :param workdir: path to directory containing tomodrgn training results
    :param outdir: path to base directory to save outputs
    :param final_epoch: the final epoch to analyze (typically the last epoch of training), zero-indexed.
            Note that because three epochs are needed to define the two inter-epoch vectors analyzed "per epoch", the output contains metrics for E-2 epochs.
            Accordingly, plot x-axes are labeled from epoch 2 - epoch E.
    :return: None
    """
    # list the metrics to be calculated on latent embeddings
    metrics = ['dot product', 'magnitude', 'cosine distance']

    # calculate latent embedding metrics
    vector_metrics = np.zeros((final_epoch - 1, len(metrics)))
    for i in np.arange(final_epoch - 1):
        z1 = utils.load_pkl(workdir + f'/z.{i}.train.pkl')
        z2 = utils.load_pkl(workdir + f'/z.{i + 1}.train.pkl')
        z3 = utils.load_pkl(workdir + f'/z.{i + 2}.train.pkl')

        diff21 = z2 - z1
        diff32 = z3 - z2

        vector_metrics[i, 0] = np.median(np.einsum('ij,ij->i', diff21, diff32), axis=0)  # median vector dot product
        vector_metrics[i, 1] = np.median(np.linalg.norm(diff32, axis=1), axis=0)  # median vector magnitude
        uv = np.sum(diff32 * diff21, axis=1)
        uu = np.sum(diff32 * diff32, axis=1)
        vv = np.sum(diff21 * diff21, axis=1)
        vector_metrics[i, 2] = np.median(1 - uv / (np.sqrt(uu) * np.sqrt(vv)))  # median vector cosine distance

    utils.save_pkl(vector_metrics, outdir + '/vector_metrics.pkl')

    # plot vector metrics
    fig, axes = plt.subplots(nrows=1, ncols=len(metrics), figsize=(10, 3))
    for (i, ax) in enumerate(axes.ravel()):
        ax.plot(np.arange(2, final_epoch + 1), vector_metrics[:, i])
        ax.set_xlabel('epoch')
        ax.set_ylabel(metrics[i])
    fig.tight_layout()
    plt.savefig(f'{outdir}/plots/03_encoder_latent_vector_shifts.png', dpi=300)
    log(f'Saved latent vector shifts plots to {outdir}/plots/03_encoder_latent_vector_shifts.png')
    plt.close()


def sketch_via_umap_local_maxima(outdir: str,
                                 sketch_epoch: int,
                                 n_bins: int = 30,
                                 smooth: bool = True,
                                 smooth_width: int = 1,
                                 pruned_maxima: int = 12,
                                 radius: int = 5,
                                 final_maxima: int = 10) -> tuple[np.ndarray, str]:
    """
    Sketch one epoch's latent space via local maxima finding to find dense neighborhoods of particles with similar embeddings as "well-supported" by the data

    :param outdir: path to base directory to save outputs
    :param sketch_epoch: epoch for which the (previously calculated) umap distribution will be sketched for local maxima
    :param n_bins: the number of bins along UMAP1 and UMAP2 to discretize when finding local maxima
    :param smooth: whether to smooth the 2D histogram (aids local maxima finding for particulaly continuous distributions)
    :param smooth_width: scalar multiple of one-bin-width defining sigma for gaussian kernel smoothing
    :param pruned_maxima: max number of local maxima above which pruning will be performed
    :param radius: radius in number of bins below which points are considered poorly-separated and are candidates for pruning
    :param final_maxima: the count of local maxima with the largest associated bin count that will be returned as candidate particles to the user
    :return: binary mask of shape ((nptcls, n_local_maxima)) labeling all particles in the bin and neighboring 8 bins of a local maxima;
            a unique letter assigned to each local maxima
    """

    def make_edges(data: np.ndarray,
                   bins: int) -> tuple[np.ndarray, np.ndarray]:
        """
        Helper function to create two 1-D arrays defining `nbins` bin edges along axes x and y

        :param data: array of 2-dimensional data values to be binned, shape (nptcls, 2)
        :param bins: number of bins to create along each of the dimensions
        :return: bin edges along x axis, bin edges along y axis
        """
        _xedges = np.linspace(data.min(axis=0)[0], data.max(axis=0)[0], bins + 1)
        _yedges = np.linspace(data.min(axis=0)[1], data.max(axis=0)[1], bins + 1)
        return _xedges, _yedges

    def find_local_maxima(data: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
        """
        Helper function to find the coordinates and values of local maxima of a 2d hist
        Evaluates local maxima using a footprint equal to 3x3 set of bins

        :param data: binned array as a 2D histogram, with shape (bins_x, bins_y)
        :return: array of bin indices corresponding to local maxima of the 2d array,
                bin values at the local maxima bins
        """
        size = 3
        footprint = np.ones((size, size))
        footprint[1, 1] = 0

        filtered = maximum_filter(data, footprint=footprint, mode='mirror')
        mask_local_maxima = data > filtered
        maxima_bins = np.asarray(np.where(mask_local_maxima)).T
        maxima_values = data[mask_local_maxima]

        return maxima_bins, maxima_values

    def gen_peaks_img(maxima_bins: np.ndarray,
                      maxima_values: np.ndarray,
                      bin_edges: tuple[np.ndarray, np.ndarray]) -> np.ndarray:
        """
        Helper function to create an array analogous to the 2D histogram that highlights just those bins detected as local maxima

        :param maxima_bins: array of bin indices corresponding to local maxima of the 2d array
        :param maxima_values: bin values at the local maxima bins
        :param bin_edges: bin edges along x axis, bin edges along y axis
        :return:
        """
        filtered = np.zeros((bin_edges[0].shape[0], bin_edges[1].shape[0]))
        for peak in range(maxima_bins.shape[0]):
            filtered[tuple(maxima_bins[peak])] = maxima_values[peak]
        return filtered

    def prune_local_maxima(maxima_bins: np.ndarray,
                           maxima_values: np.ndarray,
                           n_maxima: int,
                           bin_radius: int) -> tuple[np.ndarray, np.ndarray]:
        """
        Helper function to prune "similar" local maxima and preserve UMAP diversity if more local maxima than desired are found
        Construct distance matrix of all coords to all coords in bin-space
        Find all maxima pairs closer than `bin_radius`
        While more than `n_maxima` local maxima:
            if there are pairs closer than `bin_radius`:
                find single smallest distance d between two points
                compare points connected by d, remove lower value point from coords, values, and distance matrix

        :param maxima_bins: array of bin indices corresponding to local maxima of the 2d array
        :param maxima_values: bin values at the local maxima bins
        :param n_maxima: number of local maxima to target after pruning
        :param bin_radius: cutoff distance (in number of bins) at which to consider two bins as candidates for pruning (potentially overlapping)
        :return: array of bin indices corresponding to local maxima of the 2d array after pruning,
                bin values at the local maxima bins after pruning
        """
        dist_matrix = distance_matrix(maxima_bins, maxima_bins)
        dist_matrix[dist_matrix > bin_radius] = 0  # ignore points separated by > @radius in bin-space

        while len(maxima_values) > n_maxima:
            if not np.count_nonzero(dist_matrix) == 0:  # some peaks are too close and need pruning
                indices_to_compare = np.where(dist_matrix == np.min(dist_matrix[np.nonzero(dist_matrix)]))[0]
                if maxima_values[indices_to_compare[0]] > maxima_values[indices_to_compare[1]]:
                    dist_matrix = np.delete(dist_matrix, indices_to_compare[1], axis=0)
                    dist_matrix = np.delete(dist_matrix, indices_to_compare[1], axis=1)
                    maxima_values = np.delete(maxima_values, indices_to_compare[1])
                    maxima_bins = np.delete(maxima_bins, indices_to_compare[1], axis=0)
                else:
                    dist_matrix = np.delete(dist_matrix, indices_to_compare[0], axis=0)
                    dist_matrix = np.delete(dist_matrix, indices_to_compare[0], axis=1)
                    maxima_values = np.delete(maxima_values, indices_to_compare[0])
                    maxima_bins = np.delete(maxima_bins, indices_to_compare[0], axis=0)
            else:  # local maxima are already well separated
                return maxima_bins, maxima_values
        return maxima_bins, maxima_values

    def coords_to_umap(_umap_emb: np.ndarray,
                       _binned_ptcls_mask: np.ndarray) -> np.ndarray:
        """
        Helper function to convert local maxima coords in bin-space to umap-space
        Calculates each local maximum to be the median UMAP1 and UMAP2 value across all particles in each 3x3 set of bins defining a given local maximum

        :param _umap_emb: UMAP embeddings of all particles, shape (nptcls, 2)
        :param _binned_ptcls_mask: mask array of particles to be considered as within the specified local maxima
        :return: coordinate of median particle within each local maxima
        """
        umap_median_peaks = np.zeros((_binned_ptcls_mask.shape[1], 2))
        for j in range(_binned_ptcls_mask.shape[1]):
            umap_median_peaks[j, :] = np.median(_umap_emb[_binned_ptcls_mask[:, j], :], axis=0)
        return umap_median_peaks

    # load the previously calculated umap embeddings
    umap_emb = utils.load_pkl(f'{outdir}/umaps/umap.{sketch_epoch}.pkl')
    nptcls = umap_emb.shape[0]

    # create 2d histogram of umap distribution
    edges = make_edges(umap_emb, bins=n_bins)
    hist, xedges, yedges, bincount = stats.binned_statistic_2d(x=umap_emb[:, 0],
                                                               y=umap_emb[:, 1],
                                                               values=None,
                                                               statistic='count',
                                                               bins=edges,
                                                               expand_binnumbers=True)
    to_plot = ['umap', 'hist']

    # optionally smooth the histogram to reduce the number of peaks with sigma=width of two bins
    if smooth:
        hist_smooth = gaussian_filter(hist, smooth_width * np.abs(xedges[1] - xedges[0]))
        coords, values = find_local_maxima(hist_smooth)
        to_plot[-1] = 'hist_smooth'
    else:
        hist_smooth = None
        coords, values = find_local_maxima(hist)
    log(f'Found {len(values)} local maxima')

    # prune local maxima that are densely packed and low in value
    coords, values = prune_local_maxima(coords, values, pruned_maxima, radius)
    log(f'Pruned to {len(values)} local maxima based on local maxima being closer than {radius} units in UMAP-bin-space')

    # find subset of n_peaks highest local maxima
    indices = (-values).argsort()[:final_maxima]
    coords, values = coords[indices], values[indices]
    peaks_img_top = gen_peaks_img(coords, values, edges)
    to_plot.append('peaks_img_top')
    to_plot.append('sketched_umap')
    log(f'Filtered to top {len(values)} local maxima based on highest local maxima.')

    # write list of lists containing indices of all particles within maxima bins + all 8 neighboring bins (assumes footprint = (3,3))
    binned_ptcls_mask = np.zeros((nptcls, len(values)), dtype=bool)
    for i in range(len(values)):
        binned_ptcls_mask[:, i] = (bincount[0, :] >= coords[i, 0] + 0) & \
                                  (bincount[0, :] <= coords[i, 0] + 2) & \
                                  (bincount[1, :] >= coords[i, 1] + 0) & \
                                  (bincount[1, :] <= coords[i, 1] + 2)

    # find median umap coords of each maxima bin for plotting
    coords = coords_to_umap(umap_emb, binned_ptcls_mask)

    # plot the original histogram, all peaks, and highest n_peaks
    fig, axes = plt.subplots(1, len(to_plot), figsize=(len(to_plot) * 3.6, 3))
    fig.tight_layout()
    labels = ascii_uppercase[:len(values)]
    for (i, ax) in enumerate(axes.ravel()):
        if to_plot[i] == 'umap':
            ax.hexbin(umap_emb[:, 0], umap_emb[:, 1], mincnt=1)
            ax.vlines(x=xedges, ymin=umap_emb.min(axis=0)[1], ymax=umap_emb.max(axis=0)[1], colors='red', linewidth=0.35)
            ax.hlines(y=yedges, xmin=umap_emb.min(axis=0)[0], xmax=umap_emb.max(axis=0)[0], colors='red', linewidth=0.35)
            ax.set_title(f'epoch {sketch_epoch} UMAP')
            ax.set_xlabel('UMAP1')
            ax.set_ylabel('UMAP2')
        elif to_plot[i] == 'hist':
            ax.imshow(np.rot90(hist))
            ax.set_title('UMAP histogram')
        elif to_plot[i] == 'hist_smooth':
            ax.imshow(np.rot90(hist_smooth))
            ax.set_title('UMAP smoothed histogram')
        elif to_plot[i] == 'peaks_img_top':
            ax.imshow(np.rot90(peaks_img_top))
            ax.set_title(f'final {len(labels)} local maxima')
        elif to_plot[i] == 'sketched_umap':
            ax.hexbin(umap_emb[:, 0], umap_emb[:, 1], mincnt=1)
            ax.scatter(*coords.T, c='r')
            ax.set_title(f'sketched epoch {sketch_epoch} UMAP')
            ax.set_xlabel('UMAP1')
            ax.set_ylabel('UMAP2')
            for k in range(len(values)):
                ax.text(x=coords[k, 0] + 0.3,
                        y=coords[k, 1] + 0.3,
                        s=labels[k],
                        fontdict=dict(color='r', size=10))
        ax.spines['bottom'].set_visible(True)
        ax.spines['left'].set_visible(True)
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)

    plt.tight_layout()
    plt.savefig(f'{outdir}/plots/04_decoder_UMAP-sketching.png', dpi=300)
    log(f'Saved latent sketching plot to {outdir}/plots/04_decoder_UMAP-sketching.png')
    plt.close()

    return binned_ptcls_mask, labels


def follow_candidate_particles(workdir: str,
                               outdir: str,
                               epochs: np.ndarray,
                               binned_ptcls_mask: np.ndarray,
                               labels: str) -> None:
    """
    Plot how the labeled set of particles migrates within latent space at selected epochs over training.
    Saves plot.png tracking representative latent encodings through epochs and latent.txt of representative latent encodings for each epoch

    :param workdir: path to directory containing tomodrgn training results
    :param outdir: path to base directory to save outputs
    :param epochs: array of epochs for which to plot UMAPs
    :param binned_ptcls_mask: binary mask of shape ((nptcls, n_local_maxima)) labeling all particles in the bin and neighboring 8 bins of a local maxima
    :param labels: a unique letter assigned to each local maxima
    :return: None
    """

    # track sketched points from epoch E through selected previous epochs and plot overtop UMAP embedding
    n_cols = int(np.ceil(len(epochs) ** 0.5))
    n_rows = int(np.ceil(len(epochs) / n_cols))

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3 * n_rows), sharex='all', sharey='all')

    ind_subset = utils.load_pkl(f'{outdir}/umaps/ind_subset.pkl')
    for (i, ax) in enumerate(axes.ravel()):
        try:
            umap = utils.load_pkl(outdir + f'/umaps/umap.{epochs[i]}.pkl')
            z = utils.load_pkl(workdir + f'/z.{epochs[i]}.train.pkl')[ind_subset, :]
            zdim = z.shape[1]
            z_maxima_median = np.zeros((len(labels), zdim))

            for k in range(len(labels)):
                z_maxima_median[k, :] = np.median(z[binned_ptcls_mask[:, k]], axis=0)  # find median latent value of each maximum in a given epoch

            z_maxima_median_ondata, z_maxima_median_ondata_ind = analysis.get_nearest_point(z, z_maxima_median)  # find on-data latent encoding of each median latent value
            umap_maxima_median_ondata = umap[z_maxima_median_ondata_ind]  # find on-data UMAP embedding of each median latent encoding

            # Write out the on-data median latent values of each labeled set of particles for each epoch in epochs
            with open(outdir + f'/repr_particles/latent_representative.{epochs[i]}.txt', 'w') as f:
                np.savetxt(f, z_maxima_median_ondata, delimiter=' ', newline='\n', header='', footer='', comments='# ')
            log(f'Saved representative latent encodings for epoch {epochs[i]} to {outdir}/repr_particles/latent_representative.{epochs[i]}.txt')

            for k in range(len(labels)):
                ax.text(x=umap_maxima_median_ondata[k, 0] + 0.3,
                        y=umap_maxima_median_ondata[k, 1] + 0.3,
                        s=labels[k],
                        fontdict=dict(color='r', size=10))
            toplot = ax.hexbin(*umap.T, bins='log', mincnt=1)
            ax.scatter(umap_maxima_median_ondata[:, 0], umap_maxima_median_ondata[:, 1], s=10, linewidth=0, c='r',
                       alpha=1)
            ax.set_title(f'epoch {epochs[i]}')

            if i % n_cols == 0:
                # set y axis label only for the left column of subplots
                ax.set_ylabel('l-UMAP2')
            if i >= n_cols * (n_rows - 1):
                # set the x axis label only for the bottom row of subplots
                ax.set_xlabel('l-UMAP1')

            if i == len(epochs) - 1:
                # add a colorbar of particle density
                fig.subplots_adjust(left=0.15, right=0.8, top=0.85, bottom=0.15)
                cbar_ax = fig.add_axes((0.82, 0.15, 0.02, 0.7))
                cbar = fig.colorbar(toplot, cax=cbar_ax)
                cbar.ax.set_ylabel('particle density', rotation=90)
        except IndexError:
            # this is an extra axis for which we do not have data to plot, hide it
            ax.axis('off')

    plt.savefig(f'{outdir}/plots/05_decoder_maxima-sketch-consistency.png', dpi=300)
    log(f'Saved plot tracking representative latent encodings through epochs {epochs} to {outdir}/plots/05_decoder_maxima-sketch-consistency.png')
    plt.close()


def calc_ccs_pairwise_epochs(outdir: str,
                             epochs: np.ndarray,
                             labels: str,
                             mask: str | None = None,
                             thresh: int = 99,
                             dilate: int | None = None,
                             dist: int | None = None) -> None:
    """
    For each successive pair of epochs in `epochs`, calculates the (auto-masked) correlation coefficient between the same particle's tomoDRGN-evaluated volume at those epochs.
    Assumes that volume paths look like `outdir/vols.epoch/vol_123.mrc`.
    Assumes that vol_000 in epoch X is generated from the same particle as vol_000 in epoch Y.
    Saves a plot of the successive pairwise correlation coefficient across training epochs.

    :param outdir: path to base directory to save outputs
    :param epochs: array of epochs to analyze
    :param labels: a unique letter assigned to each local maxima particle
    :param mask: mask to calculate from volumes, one of [None, 'sphere', 'tight', 'soft']
    :param thresh: data percentile threshold at which to binarize mask when calculating 'tight' or initializing 'soft' masks
    :param dilate: for soft mask, number of pixels to expand auto-determined tight mask
    :param dist: for soft mask, number of pixels over which to apply soft edge
    :return: None
    """
    # preallocate array to store CC results
    cc_masked = np.zeros((len(labels), len(epochs) - 1))

    # calculate CCs
    for i in range(len(epochs) - 1):
        for cluster in np.arange(len(labels)):
            vol1, _ = mrc.parse_mrc(f'{outdir}/vols.{epochs[i]}/vol_{cluster:03d}.mrc')
            vol2, _ = mrc.parse_mrc(f'{outdir}/vols.{epochs[i + 1]}/vol_{cluster:03d}.mrc')
            cc_masked[cluster, i] = utils.calc_cc(vol1=vol1,
                                                  vol2=vol2,
                                                  mask=mask,
                                                  thresh=thresh,
                                                  dilate=dilate,
                                                  dist=dist)
    utils.save_pkl(cc_masked, f'{outdir}/cc_masked.pkl')

    # plot the successive pairs of epochs' CCs
    n_cols = 1
    n_rows = 1
    fig, ax = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3 * n_rows))
    colors = analysis.get_colors_chimerax(len(labels))
    for i in range(len(labels)):
        ax.plot(epochs[1:], cc_masked[i, :], c=colors[i], marker='.', )
    ax.legend(labels, ncol=3, fontsize='x-small')
    ax.set_xlabel('epoch')
    ax.set_ylabel('correlation coefficient')
    plt.tight_layout()
    plt.savefig(f'{outdir}/plots/06_decoder_CC.png', dpi=300)
    log(f'Saved map-map correlation plot to {outdir}/plots/06_decoder_CC.png')
    plt.close()


def calc_fscs_pairwise_epochs(outdir: str,
                              epochs: np.ndarray,
                              labels: str,
                              mask: str | None = None,
                              thresh: int = 99,
                              dilate: int | None = None,
                              dist: int | None = None) -> None:
    """
    For each successive pair of epochs in `epochs`, calculates the (auto-masked) FSC between the same particle's tomoDRGN-evaluated volume at those epochs.
    Assumes that volume paths look like `outdir/vols.epoch/vol_123.mrc`.
    Assumes that vol_000 in epoch X is generated from the same particle as vol_000 in epoch Y.
    Saves a plot of the successive pairwise FSC at each epoch; saves a plot of the successive pairwise FSC values at Nyquist resolution at each epoch.

    :param outdir: path to base directory to save outputs
    :param epochs: array of epochs to analyze
    :param labels: a unique letter assigned to each local maxima particle
    :param mask: mask to calculate from volumes, one of [None, 'sphere', 'tight', 'soft']
    :param thresh: data percentile threshold at which to binarize mask when calculating 'tight' or initializing 'soft' masks
    :param dilate: for soft mask, number of pixels to expand auto-determined tight mask
    :param dist: for soft mask, number of pixels over which to apply soft edge
    :return: None
    """
    # preallocate array to store FSC results
    boxsize = mrc.parse_mrc(f'{outdir}/vols.{epochs[0]}/vol_000.mrc')[0].shape[0]
    x = np.arange(0, boxsize // 2 + 1)
    fsc_masked = np.zeros((len(labels), len(epochs) - 1, boxsize // 2 + 1))

    # calculate FSCs
    for i in range(len(epochs) - 1):
        for cluster in np.arange(len(labels)):
            vol1, _ = mrc.parse_mrc(f'{outdir}/vols.{epochs[i]}/vol_{cluster:03d}.mrc')
            vol2, _ = mrc.parse_mrc(f'{outdir}/vols.{epochs[i + 1]}/vol_{cluster:03d}.mrc')
            x, fsc = utils.calc_fsc(vol1=vol1,
                                    vol2=vol2,
                                    mask=mask,
                                    thresh=thresh,
                                    dilate=dilate,
                                    dist=dist)
            fsc_masked[cluster, i, :] = fsc
    utils.save_pkl(fsc_masked, outdir + '/fsc_masked.pkl')
    utils.save_pkl(x, outdir + '/fsc_masked_frequencies.pkl')

    # plot all fscs
    n_cols = int(np.ceil(len(labels) ** 0.5))
    n_rows = int(np.ceil(len(labels) / n_cols))
    fig, axes = plt.subplots(nrows=n_rows,
                             ncols=n_cols,
                             figsize=(3 * n_cols, 3 * n_rows),
                             sharex='all',
                             sharey='all')
    for (i, ax) in enumerate(axes.ravel()):
        try:
            colors = analysis.get_colors_matplotlib(num_colors=len(epochs), cmap='viridis')
            legend = []
            for j in range(len(epochs) - 1):
                ax.plot(x, fsc_masked[i, j, :], color=colors[j])
                legend.append(f'epoch {epochs[j + 1]}')
            ax.set_ylim(0, 1.02)
            ax.set_title(f'maximum {labels[i]}')
            if i % n_cols == 0:
                # set y axis label only for the left column of subplots
                ax.set_ylabel('FSC')
            if i >= n_cols * (n_rows - 1):
                # set the x axis label only for the bottom row of subplots
                ax.set_xlabel('frequency (1/px)')
            if (i % n_cols == 0) and (i >= n_cols * (n_rows - 1)):
                ax.legend(legend, loc='lower left', ncol=2, fontsize=6.5)
        except IndexError:
            # this is an extra axis for which we do not have data to plot, hide it
            ax.axis('off')
    plt.tight_layout()
    plt.savefig(f'{outdir}/plots/07_decoder_FSC.png', dpi=300)
    log(f'Saved map-map FSC plot to {outdir}/plots/07_decoder_FSC.png')
    plt.close()

    # plot all FSCs at Nyquist only
    n_cols = 1
    n_rows = 1
    fig, ax = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3 * n_rows))
    colors = analysis.get_colors_chimerax(len(labels))
    for i in range(len(labels)):
        ax.plot(epochs[1:], fsc_masked[i, :, -1], c=colors[i])
    ax.set_xlabel('epoch')
    ax.set_ylabel('FSC at nyquist')
    ax.legend(labels, ncol=3, fontsize='x-small')
    plt.tight_layout()
    plt.savefig(f'{outdir}/plots/08_decoder_FSC-nyquist.png', dpi=300)
    log(f'Saved map-map FSC (Nyquist) plot to {outdir}/plots/08_decoder_FSC-nyquist.png')
    plt.close()


def calc_ccs_alltoall_intraepoch(outdir: str,
                                 epochs: np.ndarray,
                                 labels: str,
                                 mask: str | None = None,
                                 thresh: int = 99,
                                 dilate: int | None = None,
                                 dist: int | None = None) -> None:
    """
    For each epoch in `epochs`, calculates the (auto-masked) correlation coefficient between each pair of volumes generated at that epoch.
    Assumes that volume paths look like `outdir/vols.epoch/vol_123.mrc`.
    Saves a clustermap of the all-to-all correlation coefficients at each epoch.

    :param outdir: path to base directory to save outputs
    :param epochs: array of epochs to analyze
    :param labels: a unique letter assigned to each local maxima particle
    :param mask: mask to calculate from volumes, one of [None, 'sphere', 'tight', 'soft']
    :param thresh: data percentile threshold at which to binarize mask when calculating 'tight' or initializing 'soft' masks
    :param dilate: for soft mask, number of pixels to expand auto-determined tight mask
    :param dist: for soft mask, number of pixels over which to apply soft edge
    :return: None
    """
    # preallocate array to store CC results
    cc_alltoall = np.ones((len(epochs), len(labels), len(labels)))  # using np.ones here because we skip self-self (on-diagonal) CC calculations, which should have value of 1

    # calculate the CCs
    for i in range(len(epochs)):
        # skip symmetric-equivalent and self-self CC calculations
        efficient_matrix_inds = np.triu_indices(len(labels), 1)
        inds_a, inds_b = efficient_matrix_inds

        # calculate the CCs
        vols = [f'{outdir}/vols.{epochs[i]}/vol_{cluster:03d}.mrc' for cluster in range(len(labels))]
        for ind_a, ind_b in zip(inds_a, inds_b):
            cc_alltoall[i, ind_a, ind_b] = utils.calc_cc(vol1=vols[ind_a],
                                                         vol2=vols[ind_b],
                                                         mask=mask,
                                                         thresh=thresh,
                                                         dilate=dilate,
                                                         dist=dist)
            cc_alltoall[i, ind_b, ind_a] = cc_alltoall[i, ind_a, ind_b]

        # visualize and save as a heatmap
        df = pd.DataFrame(cc_alltoall[i], index=[label for label in labels], columns=[label for label in labels])
        sns.clustermap(df, annot=True, fmt='0.2f', figsize=(6, 6), vmin=0.75, vmax=1.0)
        plt.savefig(f'{outdir}/plots/temp_{i}.png', dpi=300, format='png', transparent=True, bbox_inches='tight')
        plt.close()

    utils.save_pkl(cc_alltoall, outdir + '/cc_masked_alltoall.pkl')

    # plot all CC clustermaps
    n_cols = int(np.ceil(len(epochs) ** 0.5))
    n_rows = int(np.ceil(len(epochs) / n_cols))
    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(3 * n_cols, 3 * n_rows), sharex='all', sharey='all')
    fig.tight_layout()
    for (i, ax) in enumerate(axes.ravel()):
        path = f'{outdir}/plots/temp_{i}.png'
        if os.path.isfile(path):
            ax.imshow(image.imread(path))
            ax.set_title(f'epoch {epochs[i]}')
            os.remove(path)
        else:
            pass
        # do not display any axes since we are hacking to show each previously generated clustermap as an image
        ax.set_axis_off()
    plt.tight_layout()
    plt.savefig(f'{outdir}/plots/09_pairwise_CC_matrix_epoch.png', dpi=300)
    log(f'Saved pairwise map-map CC clustermap to {outdir}/plots/09_pairwise_CC_matrix_epoch.png')
    plt.close()


def calc_ccs_alltogroundtruth(outdir: str,
                              epochs: np.ndarray,
                              labels: str,
                              ground_truth_paths: list[str],
                              mask: str | None = None,
                              thresh: int = 99,
                              dilate: int | None = None,
                              dist: int | None = None) -> None:
    """
    For each epoch in `epochs`, calculates the (auto-masked) correlation coefficient between each tomoDRGN volume and each ground_truth volume.
    Assumes that volume paths look like `outdir/vols.epoch/vol_123.mrc`.
    Saves a clustermap of the correlation coefficients at each epoch.

    :param outdir: path to base directory to save outputs
    :param epochs: array of epochs to analyze
    :param labels: a unique letter assigned to each local maxima particle
    :param ground_truth_paths: list of paths to ground truth volumes
    :param mask: mask to calculate from volumes, one of [None, 'sphere', 'tight', 'soft']
    :param thresh: data percentile threshold at which to binarize mask when calculating 'tight' or initializing 'soft' masks
    :param dilate: for soft mask, number of pixels to expand auto-determined tight mask
    :param dist: for soft mask, number of pixels over which to apply soft edge
    :return: None
    """
    # preallocate array to store CC results
    cc_voltogt = np.zeros((len(epochs), len(ground_truth_paths), len(labels)))

    # get all ground truth vols
    gt_vols = sorted(ground_truth_paths)
    gt_labels = [os.path.splitext(os.path.basename(gt_vol))[0] for gt_vol in gt_vols]

    # calculate the CCs
    for i in range(len(epochs)):
        tomodrgn_vols = [f'{outdir}/vols.{epochs[i]}/vol_{cluster:03d}.mrc' for cluster in range(len(labels))]
        for j in range(len(gt_vols)):
            for k in range(len(tomodrgn_vols)):
                cc_voltogt[i, j, k] = utils.calc_cc(vol1=gt_vols[j],
                                                    vol2=tomodrgn_vols[k],
                                                    mask=mask,
                                                    thresh=thresh,
                                                    dilate=dilate,
                                                    dist=dist)

        # visualize and save as a heatmap
        df = pd.DataFrame(cc_voltogt[i], index=[label for label in gt_labels], columns=[label for label in labels])
        sns.clustermap(df, annot=True, fmt='0.2f', figsize=(len(tomodrgn_vols), len(gt_vols)), vmin=0, vmax=1.0, row_cluster=False)
        plt.tight_layout()
        plt.savefig(f'{outdir}/plots/temp_{i}.png', dpi=300)
        plt.close()

    utils.save_pkl(cc_voltogt, f'{outdir}/cc_masked_alltogroundtruth.pkl')

    # plot all CC clustermaps
    n_cols = int(np.ceil(len(epochs) ** 0.5))
    n_rows = int(np.ceil(len(epochs) / n_cols))
    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(3 * n_cols, 3 * n_rows), sharex='all', sharey='all')
    fig.tight_layout()
    for (i, ax) in enumerate(axes.ravel()):
        path = f'{outdir}/plots/temp_{i}.png'
        if os.path.isfile(path):
            ax.imshow(image.imread(path))
            ax.set_title(f'epoch {epochs[i]}')
            os.remove(path)
        else:
            pass
        # do not display any axes since we are hacking to show each previously generated clustermap as an image
        ax.set_axis_off()
    fig.tight_layout()
    plt.savefig(f'{outdir}/plots/10_groundtruth_CC_matrix_epoch.png', dpi=300 * n_cols)
    log(f'Saved ground truth map-map CC clustermap to {outdir}/plots/10_groundtruth_CC_matrix_epoch.png')
    plt.close()


def calc_kld_two_gaussians(z_mu_train: np.ndarray,
                           z_logvar_train: np.ndarray,
                           z_mu_test: np.ndarray,
                           z_logvar_test: np.ndarray,
                           workdir: str,
                           epoch: int) -> None:
    """
    Compute the KLD between two gaussians with diagomal covariance.
    Used to test convergence via difference between each particle's train and test embedding for selected epoch.

    :param z_mu_train: numpy array of shape (n_particles, zdim), latent embedding means deriving from particle train images
    :param z_logvar_train: numpy array of shape (n_particles, zdim), latent embedding log variance deriving from particle train images
    :param z_mu_test: numpy array of shape (n_particles, zdim), latent embedding means deriving from particle test images
    :param z_logvar_test: numpy array of shape (n_particles, zdim), latent embedding log variance deriving from particle test images
    :param workdir: str, absolute path to model workdir
    :param epoch: int, current epoch number
    :return: None
    """

    def kld_twodiagonalgaussians(mu1: np.ndarray,
                                 var1: np.ndarray,
                                 mu2: np.ndarray,
                                 var2: np.ndarray) -> np.ndarray:
        """
        Calculate the KLD between two multidimensional gaussians with diagonal covariance matrices
        Adapted from https://stackoverflow.com/a/55688087

        KL( (mu1, var1) || (mu2, var2))
             = 0.5 * ( tr(var2^{-1} var1) + log |var2|/|var1| + (mu2 - mu1)^T var2^{-1} (mu2 - mu1) - N )  (general)
             = 0.5 * ( sum(var1 / var2) + log( prod(var2) / prod(var1)) + (mu2 - mu1)**2 / var2) - N )     (diagonal covariance)
        """
        zdim = mu1.shape[-1]

        trace_term = np.sum(var1 / var2, axis=-1)
        determinant_term = np.log(np.prod(var2, axis=-1) / np.prod(var1, axis=-1))
        quad_term = np.sum((mu2 - mu1) ** 2 / var2, axis=-1)

        _kld = 0.5 * (trace_term + determinant_term + quad_term - zdim)
        return _kld

    # calculate klds
    kld = kld_twodiagonalgaussians(z_mu_train, np.exp(z_logvar_train), z_mu_test, np.exp(z_logvar_test))
    utils.save_pkl(kld, f'{workdir}/convergence_latent_kld.{epoch}.pkl')
    log(f'Convergence epoch {epoch}: latent: 90th percentile of KLD: {np.percentile(kld, 90)}')

    try:
        kld_previous = utils.load_pkl(f'{workdir}/convergence_latent_kld.{epoch - 1}.pkl')
        dist_from_previous = np.mean(kld - kld_previous)
        log(f'Convergence epoch {epoch}: latent: average change from previous epoch KLD: {dist_from_previous}')
    except FileNotFoundError:
        pass


def generate_test_train_pair_volumes(z_train: np.ndarray,
                                     z_test: np.ndarray,
                                     epoch: int,
                                     workdir: str,
                                     volume_count=100) -> None:
    """
    Select random particle indices and generate corresponding volumes using train-split images' latent embeddings and test-split latent embeddings.

    :param z_train: numpy array of shape (n_particles, zdim) of latent values deriving from train particle images
    :param z_test: numpy array of shape (n_particles) zdim) of latent values deriving from test particle images
    :param epoch: epoch being analyzed
    :param workdir: str, absolute path to model workdir to store generated volumes
    :param volume_count: int, number of volumes to generate for each of train/test
    :return: None
    """
    # sample volume_count particles
    n_particles = z_train.shape[0]
    ind_sel = np.sort(np.random.choice(n_particles, size=volume_count))

    # generate the train and test volumes for corresponding particles
    vg = models.VolumeGenerator(config=f'{workdir}/config.pkl',
                                weights_path=f'{workdir}/weights.{epoch}.pkl')
    vg.generate_volumes(z=z_train[ind_sel],
                        out_dir=f'{workdir}/scratch.{epoch}.train')
    vg.generate_volumes(z=z_test[ind_sel],
                        out_dir=f'{workdir}/scratch.{epoch}.test')

    # save data
    utils.save_pkl(data=ind_sel,
                   out_pkl=f'{workdir}/convergence_volumes_sel.{epoch}.pkl')


def calc_test_train_pair_volumes_fscs(workdir: str,
                                      epoch: int) -> None:
    """
    Calculate the FSC between volumes generated from test and train latent embeddings of the same particles in the same epoch.
    Assumes volumes are previously generated and stored in `workdir/scratch.epoch.train/` and `workdir/scratch.epoch.test/`

    :param workdir: str, absolute path to model workdir
    :param epoch: int, current epoch being evaluated
    :return: None
    """
    # calculate pairwise FSC and return resolution of FSC=0.5
    volume_count = len(glob.glob(f'{workdir}/scratch.{epoch}.train/vol_*.mrc'))
    resolutions_point5 = np.zeros(volume_count)
    fscs = []
    for i in range(volume_count):
        vol_train = f'{workdir}/scratch.{epoch}.train/vol_{i:03d}.mrc'
        vol_test = f'{workdir}/scratch.{epoch}.test/vol_{i:03d}.mrc'
        x, fsc = utils.calc_fsc(vol_train, vol_test, mask='soft')
        resolutions_point5[i] = x[-1] if np.all(fsc >= 0.5) else x[np.argmax(fsc < 0.5)]
        fscs.append(fsc)

    # save data and log summary value
    utils.save_pkl(np.asarray(fscs), f'{workdir}/convergence_volumes_testtrain_correlation.{epoch}.pkl')
    log(f'Convergence epoch {epoch}: volume correlation: 90th percentile of test/train map-map FSC 0.5 resolutions (units: 1/px): {np.percentile(resolutions_point5, 90)}')

    try:
        # fscs_previous = utils.load_pkl(f'{workdir}/convergence_volumes_testtrain_correlation.{epoch - 1}.pkl')
        # resolutions_point5_previous = [x[-1] if np.all(fsc >= 0.5) else x[np.argmax(fsc < 0.5)] for fsc in fscs_previous]
        dist_from_previous = None  # TODO KLD between two distributinons (or wasserstein?)
        log(f'Convergence epoch {epoch}: volume correlation: average change from previous epoch KLD: {dist_from_previous}')
    except FileNotFoundError:
        pass


def calc_test_train_pair_volumes_cc_complement(workdir: str,
                                               epoch: int) -> None:
    """
    Calculate the scale of heterogeneity among all volumes generated from test latent embeddings of the same particles in the same epoch

    :param workdir: str, absolute path to model workdir
    :param epoch: int, current epoch being evaluated
    :return: None
    """
    volume_count = len(glob.glob(f'{workdir}/scratch.{epoch}.train/vol_*.mrc'))

    # initialize empty pairwise distance matrix for CCs and corresponding upper triangle (non-redundant) pairwise indices
    pairwise_ccs = np.zeros((volume_count, volume_count))
    ind_triu = np.triu_indices(n=volume_count, k=1, m=volume_count)
    row_ind, col_ind = ind_triu

    # iterate through zipped indices, loading volumes and calculating CC
    from tomodrgn.mrc import parse_mrc
    from tomodrgn.utils import calc_cc
    for i, j in zip(row_ind, col_ind):
        vol_train, vol_train_header = parse_mrc(f'{workdir}/scratch.{epoch}.train/vol_{i:03d}.mrc')
        vol_test, vol_test_header = parse_mrc(f'{workdir}/scratch.{epoch}.test/vol_{j:03d}.mrc')
        pairwise_ccs[i, j] = calc_cc(vol_train, vol_test)

    # calculate pairwise volume distances as 1 - CC
    pairwise_cc_dist = np.ones_like(pairwise_ccs) - pairwise_ccs

    # save data and log summary value
    utils.save_pkl(pairwise_cc_dist, f'{workdir}/convergence_volumes_testtest_scale.{epoch}.pkl')
    log(f'Convergence epoch {epoch}: sum of complement-CC distances for all pairwise test/test volumes: {np.sum(pairwise_cc_dist[ind_triu])}')
    # TODO test train/test convergence metrics and determine if useful or if need further changes
