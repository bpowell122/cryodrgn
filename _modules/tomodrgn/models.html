
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>tomodrgn.models &#8212; tomoDRGN 0.2.3.dev328 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../../_static/documentation_options.js?v=d6c7774d"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/tomodrgn/models';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">tomoDRGN v0.2.3.dev328</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../overview/index.html">
    Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../tutorials/index.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../command_usage/index.html">
    Command Usage
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../api/index.html">
    API
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../overview/index.html">
    Overview
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../tutorials/index.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../command_usage/index.html">
    Command Usage
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../api/index.html">
    API
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">tomodrgn.models</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for tomodrgn.models</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Classes for creating, loading, training, and evaluating pytorch models.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">multiprocessing</span> <span class="kn">import</span> <span class="n">Pool</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Literal</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torchinfo</span>
<span class="kn">from</span> <span class="nn">einops</span> <span class="kn">import</span> <span class="n">repeat</span><span class="p">,</span> <span class="n">pack</span>
<span class="kn">from</span> <span class="nn">einops.layers.torch</span> <span class="kn">import</span> <span class="n">Reduce</span><span class="p">,</span> <span class="n">Rearrange</span>
<span class="kn">from</span> <span class="nn">torch.amp</span> <span class="kn">import</span> <span class="n">autocast</span>

<span class="kn">from</span> <span class="nn">tomodrgn</span> <span class="kn">import</span> <span class="n">utils</span><span class="p">,</span> <span class="n">lattice</span><span class="p">,</span> <span class="n">set_transformer</span><span class="p">,</span> <span class="n">fft</span><span class="p">,</span> <span class="n">mrc</span>

<span class="n">log</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">log</span>


<div class="viewcode-block" id="TiltSeriesHetOnlyVAE">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.TiltSeriesHetOnlyVAE.html#tomodrgn.models.TiltSeriesHetOnlyVAE">[docs]</a>
<span class="k">class</span> <span class="nc">TiltSeriesHetOnlyVAE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># TODO move sequential tilt sampling from dataset.__getitem__ to encoder A -&gt; B transition</span>
    <span class="c1"># TODO add torch.compile here and to other common functions (calc ctf? eval vol batch? ffts?) https://discuss.pytorch.org/t/how-should-i-use-torch-compile-properly/179021</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A module to encode multiple tilt images of a particle to a learned low-dimensional latent space embedding,</span>
<span class="sd">    then decode spatial frequency coordinates to corresponding voxel amplitudes conditioned on the per-particle latent embedding.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># No pose inference</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">hidden_layers_a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">hidden_dim_a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">out_dim_a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">ntilts</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">hidden_layers_b</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">hidden_dim_b</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">zdim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">hidden_layers_decoder</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">hidden_dim_decoder</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">lat</span><span class="p">:</span> <span class="n">lattice</span><span class="o">.</span><span class="n">Lattice</span><span class="p">,</span>
                 <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span> <span class="o">|</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
                 <span class="n">enc_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">pooling_function</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;concatenate&#39;</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;median&#39;</span><span class="p">,</span> <span class="s1">&#39;set_encoder&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span>
                 <span class="n">feat_sigma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">num_seeds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
                 <span class="n">layer_norm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">pe_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;geom_ft&#39;</span><span class="p">,</span> <span class="s1">&#39;geom_full&#39;</span><span class="p">,</span> <span class="s1">&#39;geom_lowf&#39;</span><span class="p">,</span> <span class="s1">&#39;geom_nohighf&#39;</span><span class="p">,</span> <span class="s1">&#39;linear_lowf&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="s1">&#39;none&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;geom_lowf&#39;</span><span class="p">,</span>
                 <span class="n">pe_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>

        <span class="c1"># initialize the parent nn.Module class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># save attributes used elsewhere</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc_mask</span> <span class="o">=</span> <span class="n">enc_mask</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span> <span class="o">=</span> <span class="n">in_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ntilts</span> <span class="o">=</span> <span class="n">ntilts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zdim</span> <span class="o">=</span> <span class="n">zdim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lat</span> <span class="o">=</span> <span class="n">lat</span>

        <span class="c1"># instantiate the encoder module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TiltSeriesEncoder</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="n">in_dim</span><span class="p">,</span>
                                         <span class="n">hidden_layers_a</span><span class="o">=</span><span class="n">hidden_layers_a</span><span class="p">,</span>
                                         <span class="n">hidden_dim_a</span><span class="o">=</span><span class="n">hidden_dim_a</span><span class="p">,</span>
                                         <span class="n">out_dim_a</span><span class="o">=</span><span class="n">out_dim_a</span><span class="p">,</span>
                                         <span class="n">hidden_layers_b</span><span class="o">=</span><span class="n">hidden_layers_b</span><span class="p">,</span>
                                         <span class="n">hidden_dim_b</span><span class="o">=</span><span class="n">hidden_dim_b</span><span class="p">,</span>
                                         <span class="n">out_dim</span><span class="o">=</span><span class="n">zdim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
                                         <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
                                         <span class="n">ntilts</span><span class="o">=</span><span class="n">ntilts</span><span class="p">,</span>
                                         <span class="n">pooling_function</span><span class="o">=</span><span class="n">pooling_function</span><span class="p">,</span>
                                         <span class="n">num_seeds</span><span class="o">=</span><span class="n">num_seeds</span><span class="p">,</span>
                                         <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                         <span class="n">layer_norm</span><span class="o">=</span><span class="n">layer_norm</span><span class="p">)</span>

        <span class="c1"># instantiate the decoder module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">FTPositionalDecoder</span><span class="p">(</span><span class="n">boxsize_ht</span><span class="o">=</span><span class="n">lat</span><span class="o">.</span><span class="n">boxsize</span><span class="p">,</span>
                                           <span class="n">in_dim</span><span class="o">=</span><span class="mi">3</span> <span class="o">+</span> <span class="n">zdim</span><span class="p">,</span>
                                           <span class="n">hidden_layers</span><span class="o">=</span><span class="n">hidden_layers_decoder</span><span class="p">,</span>
                                           <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim_decoder</span><span class="p">,</span>
                                           <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
                                           <span class="n">pe_type</span><span class="o">=</span><span class="n">pe_type</span><span class="p">,</span>
                                           <span class="n">pe_dim</span><span class="o">=</span><span class="n">pe_dim</span><span class="p">,</span>
                                           <span class="n">feat_sigma</span><span class="o">=</span><span class="n">feat_sigma</span><span class="p">)</span>

<div class="viewcode-block" id="TiltSeriesHetOnlyVAE.load">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.TiltSeriesHetOnlyVAE.html#tomodrgn.models.TiltSeriesHetOnlyVAE.load">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span>
             <span class="n">config</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">dict</span><span class="p">,</span>
             <span class="n">weights</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
             <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructor method to create an TiltSeriesHetOnlyVAE object from a config.pkl.</span>

<span class="sd">        :param config: Path to config.pkl or loaded config.pkl</span>
<span class="sd">        :param weights: Path to weights.pkl</span>
<span class="sd">        :param device: `torch.device` object</span>
<span class="sd">        :return: TiltSeriesHetOnlyVAE instance, Lattice instance</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># load the config dict if not preloaded</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">load_pkl</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">str</span> <span class="k">else</span> <span class="n">config</span>

        <span class="c1"># create the Lattice object</span>
        <span class="n">lat</span> <span class="o">=</span> <span class="n">lattice</span><span class="o">.</span><span class="n">Lattice</span><span class="p">(</span><span class="n">boxsize</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;lattice_args&#39;</span><span class="p">][</span><span class="s1">&#39;boxsize&#39;</span><span class="p">],</span>
                              <span class="n">extent</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;lattice_args&#39;</span><span class="p">][</span><span class="s1">&#39;extent&#39;</span><span class="p">],</span>
                              <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># create the TiltSeriesHetOnlyVAE object</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span> <span class="s2">&quot;leaky_relu&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">}[</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;activation&#39;</span><span class="p">]]</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">TiltSeriesHetOnlyVAE</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;in_dim&#39;</span><span class="p">],</span>
                                     <span class="n">hidden_layers_a</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;qlayersA&#39;</span><span class="p">],</span>
                                     <span class="n">hidden_dim_a</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;qdimA&#39;</span><span class="p">],</span>
                                     <span class="n">out_dim_a</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;out_dimA&#39;</span><span class="p">],</span>
                                     <span class="n">ntilts</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;ntilts&#39;</span><span class="p">],</span>
                                     <span class="n">hidden_layers_b</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;qlayersB&#39;</span><span class="p">],</span>
                                     <span class="n">hidden_dim_b</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;qdimB&#39;</span><span class="p">],</span>
                                     <span class="n">zdim</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;zdim&#39;</span><span class="p">],</span>
                                     <span class="n">hidden_layers_decoder</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;players&#39;</span><span class="p">],</span>
                                     <span class="n">hidden_dim_decoder</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;pdim&#39;</span><span class="p">],</span>
                                     <span class="n">lat</span><span class="o">=</span><span class="n">lat</span><span class="p">,</span>
                                     <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
                                     <span class="n">enc_mask</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;enc_mask&#39;</span><span class="p">],</span>
                                     <span class="n">pooling_function</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;pooling_function&#39;</span><span class="p">],</span>
                                     <span class="n">feat_sigma</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;feat_sigma&#39;</span><span class="p">],</span>
                                     <span class="n">num_seeds</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;num_seeds&#39;</span><span class="p">],</span>
                                     <span class="n">num_heads</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;num_heads&#39;</span><span class="p">],</span>
                                     <span class="n">layer_norm</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;layer_norm&#39;</span><span class="p">],</span>
                                     <span class="n">pe_type</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;pe_type&#39;</span><span class="p">],</span>
                                     <span class="n">pe_dim</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;pe_dim&#39;</span><span class="p">],</span> <span class="p">)</span>

        <span class="c1"># load weights if provided</span>
        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ckpt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">ckpt</span><span class="p">[</span><span class="s1">&#39;model_state_dict&#39;</span><span class="p">])</span>

        <span class="c1"># move the model to the requested device</span>
        <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">lat</span></div>


<div class="viewcode-block" id="TiltSeriesHetOnlyVAE.encode">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.TiltSeriesHetOnlyVAE.html#tomodrgn.models.TiltSeriesHetOnlyVAE.encode">[docs]</a>
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Encode the input batch of particle&#39;s tilt images to a corresponding batch of latent embeddings.</span>
<span class="sd">        Input images are masked by `self.enc_mask` if provided.</span>

<span class="sd">        :param batch: batch of particle tilt images to encode, shape (batch, ntilts, boxsize*boxsize)</span>
<span class="sd">        :return: `mu`: batch of mean values parameterizing latent embedding as Gaussian, shape (batch, zdim).</span>
<span class="sd">                `logvar`: batch of log variance values parameterizing latent embedding as Gaussian, shape (batch, zdim).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_mask</span><span class="p">]</span>  <span class="c1"># B x ntilts x D*D[mask]</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>  <span class="c1"># B x zdim*2</span>

        <span class="k">return</span> <span class="n">z</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">zdim</span><span class="p">],</span> <span class="n">z</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zdim</span><span class="p">:]</span>  <span class="c1"># B x zdim</span></div>


<div class="viewcode-block" id="TiltSeriesHetOnlyVAE.decode">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.TiltSeriesHetOnlyVAE.html#tomodrgn.models.TiltSeriesHetOnlyVAE.decode">[docs]</a>
    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">coords</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
               <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Decode a batch of lattice coordinates concatenated with the corresponding latent embedding to infer the associated voxel intensities.</span>

<span class="sd">        :param coords: 3-D spatial frequency coordinates (e.g. from Lattice.coords) concatenated with the</span>
<span class="sd">                shape (batch, ntilts * boxsize_ht * boxsize_ht [mask], 3).</span>
<span class="sd">        :param z: latent embedding per-particle, shape (batch, zdim)</span>
<span class="sd">        :return: Decoded voxel intensities at the specified 3-D spatial frequencies.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">coords</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span></div>


<div class="viewcode-block" id="TiltSeriesHetOnlyVAE.print_model_info">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.TiltSeriesHetOnlyVAE.html#tomodrgn.models.TiltSeriesHetOnlyVAE.print_model_info">[docs]</a>
    <span class="k">def</span> <span class="nf">print_model_info</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Wrapper around torchinfo to display summary of model layers, input and output tensor shapes, and number of trainable parameters.</span>
<span class="sd">        Note that the predicted model size assumes float32 input tensors and model weights, which is an overestimate by ~2x if AMP is enabled.</span>

<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># print the encoder module which we know input size exactly due to fixed ntilt sampling</span>
        <span class="n">torchinfo</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">,</span>
                          <span class="n">input_size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ntilts</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span><span class="p">),</span>
                          <span class="n">batch_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                          <span class="n">col_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;input_size&#39;</span><span class="p">,</span> <span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="s1">&#39;num_params&#39;</span><span class="p">),</span>
                          <span class="n">depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="c1"># print the decoder module which will be a conservative overestimate of input size without lattice masking</span>
        <span class="c1"># this is because each particle has a potentially unique lattice mask and therefore this is an upper bound on model size</span>
        <span class="n">torchinfo</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">,</span>
                          <span class="n">input_data</span><span class="o">=</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ntilts</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lat</span><span class="o">.</span><span class="n">boxsize</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lat</span><span class="o">.</span><span class="n">boxsize</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">zdim</span><span class="p">)],</span>
                          <span class="n">batch_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                          <span class="n">col_names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;input_size&#39;</span><span class="p">,</span> <span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="s1">&#39;num_params&#39;</span><span class="p">),</span>
                          <span class="n">depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="FTPositionalDecoder">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.FTPositionalDecoder.html#tomodrgn.models.FTPositionalDecoder">[docs]</a>
<span class="k">class</span> <span class="nc">FTPositionalDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A module to decode a (batch of tilts of) spatial frequency coordinates spanning (-0.5, 0.5) to the corresponding spatial frequency amplitude.</span>
<span class="sd">    The output may optionally be conditioned on a latent embedding `z`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">boxsize_ht</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                 <span class="n">hidden_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                 <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
                 <span class="n">activation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
                 <span class="n">pe_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;geom_ft&#39;</span><span class="p">,</span> <span class="s1">&#39;geom_full&#39;</span><span class="p">,</span> <span class="s1">&#39;geom_lowf&#39;</span><span class="p">,</span> <span class="s1">&#39;geom_nohighf&#39;</span><span class="p">,</span> <span class="s1">&#39;linear_lowf&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="s1">&#39;none&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;geom_lowf&#39;</span><span class="p">,</span>
                 <span class="n">pe_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">feat_sigma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create the FTPositionalDecoder module.</span>

<span class="sd">        :param boxsize_ht: fourier-symmetrized box width in pixels (typically odd, 1px larger than input image)</span>
<span class="sd">        :param in_dim: number of dimensions of input coordinate lattice, typically 3 (x,y,z) + zdim</span>
<span class="sd">        :param hidden_layers: number of intermediate hidden layers in the decoder module</span>
<span class="sd">        :param hidden_dim: number of features in each hidden layer in the decoder module</span>
<span class="sd">        :param activation: activation function to be applied after each layer, either `torch.nn.ReLU` or `torch.nn.LeakyReLU`</span>
<span class="sd">        :param pe_type: the type of positional encoding to map each spatial frequency coordinate (x,y,z) to a higher dimensional representation to be passed to the decoder.</span>
<span class="sd">        :param pe_dim: the dimension of the higher dimensional representation of the positional encoding, typically automatically set to half of the box size to sample up to Nyquist.</span>
<span class="sd">        :param feat_sigma: the scale of random frequency vectors sampled from a gaussian for pe_type gaussian.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># initialize the parent nn.Module class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># sanity check inputs</span>
        <span class="k">assert</span> <span class="n">in_dim</span> <span class="o">&gt;=</span> <span class="mi">3</span>

        <span class="c1"># determine the latent dimensionality as the input dimensionality minus the expected three spatial dimensions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zdim</span> <span class="o">=</span> <span class="n">in_dim</span> <span class="o">-</span> <span class="mi">3</span>

        <span class="c1"># commonly referenced boxsize-related constants</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">boxsize_ht</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">D2</span> <span class="o">=</span> <span class="n">boxsize_ht</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">DD</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">boxsize_ht</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># what type of positional encoding to use</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pe_type</span> <span class="o">=</span> <span class="n">pe_type</span>

        <span class="c1"># the dimensionality of the positional encoding defaults to half of the input coordinate box size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pe_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">D2</span> <span class="k">if</span> <span class="n">pe_dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">pe_dim</span>

        <span class="c1"># the final number of features the decoder network receives as input per 3-D coordinate to evaluate</span>
        <span class="c1"># each of the 3 spatial frequency axes will have pe_dim features expressed as both sin and cos, finally concatenated with latent embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_dim</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">zdim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layers</span> <span class="o">=</span> <span class="n">hidden_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>

        <span class="c1"># construct the decoder ResidLinearMLP module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">ResidLinearMLP</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span><span class="p">,</span>
                                      <span class="n">nlayers</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span>
                                      <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                                      <span class="n">out_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                      <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_type</span> <span class="o">==</span> <span class="s1">&#39;geom_ft&#39;</span><span class="p">:</span>
            <span class="c1"># option 1: 2/D to 1</span>
            <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pe_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">freqs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">DD</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">DD</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">freqs</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pe_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_type</span> <span class="o">==</span> <span class="s1">&#39;geom_full&#39;</span><span class="p">:</span>
            <span class="c1"># option 2: 2/D to 2pi</span>
            <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pe_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">freqs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">DD</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">DD</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">freqs</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pe_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_type</span> <span class="o">==</span> <span class="s1">&#39;geom_lowf&#39;</span><span class="p">:</span>
            <span class="c1"># option 3: 2/D*2pi to 2pi</span>
            <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pe_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">freqs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">D2</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">D2</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">freqs</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pe_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_type</span> <span class="o">==</span> <span class="s1">&#39;geom_nohighf&#39;</span><span class="p">:</span>
            <span class="c1"># option 4: 2/D*2pi to 1</span>
            <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pe_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">freqs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">D2</span> <span class="o">*</span> <span class="p">(</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">D2</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">freqs</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pe_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_type</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
            <span class="c1"># We construct 3 * self.pe_dim random vector frequences, to match the original positional encoding:</span>
            <span class="c1"># In the positional encoding we produce self.pe_dim features for each of the x,y,z dimensions,</span>
            <span class="c1"># whereas in gaussian encoding we produce self.pe_dim features each with random x,y,z components</span>
            <span class="c1">#</span>
            <span class="c1"># Each of the random feats is the sine/cosine of the dot product of the coordinates with a frequency</span>
            <span class="c1"># vector sampled from a gaussian with std of feat_sigma</span>
            <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">feat_sigma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">D2</span>
            <span class="c1"># make rand_feats a parameter so that it is saved in the checkpoint, but do not perform SGD on it</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">freqs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">freqs</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_type</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
            <span class="c1"># construct a linear increase in frequency, i.e. cos(k*n/N), sin(k*n/N), n=0,...,N//2</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">D2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Invalid pe_type </span><span class="si">{</span><span class="n">pe_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<div class="viewcode-block" id="FTPositionalDecoder.load">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.FTPositionalDecoder.html#tomodrgn.models.FTPositionalDecoder.load">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span>
             <span class="n">config</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">dict</span><span class="p">,</span>
             <span class="n">weights</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
             <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructor method to create an FTPositionalDecoder object from a config.pkl.</span>

<span class="sd">        :param config: Path to config.pkl or loaded config.pkl</span>
<span class="sd">        :param weights: Path to weights.pkl</span>
<span class="sd">        :param device: `torch.device` object</span>
<span class="sd">        :return: FTPositionalDecoder instance, Lattice instance</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># load the config dict if not preloaded</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">load_pkl</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">str</span> <span class="k">else</span> <span class="n">config</span>

        <span class="c1"># create the Lattice object</span>
        <span class="n">lat</span> <span class="o">=</span> <span class="n">lattice</span><span class="o">.</span><span class="n">Lattice</span><span class="p">(</span><span class="n">boxsize</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;lattice_args&#39;</span><span class="p">][</span><span class="s1">&#39;boxsize&#39;</span><span class="p">],</span>
                              <span class="n">extent</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;lattice_args&#39;</span><span class="p">][</span><span class="s1">&#39;extent&#39;</span><span class="p">],</span>
                              <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># create the FTPositionalDecoder object</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span> <span class="s2">&quot;leaky_relu&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">}[</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;activation&#39;</span><span class="p">]]</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">FTPositionalDecoder</span><span class="p">(</span><span class="n">boxsize_ht</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;lattice_args&#39;</span><span class="p">][</span><span class="s1">&#39;boxsize&#39;</span><span class="p">],</span>
                                    <span class="n">in_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                    <span class="n">hidden_layers</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;players&#39;</span><span class="p">],</span>
                                    <span class="n">hidden_dim</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;pdim&#39;</span><span class="p">],</span>
                                    <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
                                    <span class="n">pe_type</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;pe_type&#39;</span><span class="p">],</span>
                                    <span class="n">pe_dim</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;pe_dim&#39;</span><span class="p">],</span>
                                    <span class="n">feat_sigma</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;feat_sigma&#39;</span><span class="p">])</span>

        <span class="c1"># load weights if provided</span>
        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ckpt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">ckpt</span><span class="p">[</span><span class="s1">&#39;model_state_dict&#39;</span><span class="p">])</span>

        <span class="c1"># move the model to the requested device</span>
        <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">lat</span></div>


<div class="viewcode-block" id="FTPositionalDecoder.positional_encoding">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.FTPositionalDecoder.html#tomodrgn.models.FTPositionalDecoder.positional_encoding">[docs]</a>
    <span class="k">def</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                            <span class="n">coords</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Expand coordinates in the Fourier basis with variably spaced wavelengths</span>

<span class="sd">        :param coords: Tensor or NestedTensor shape (batch, ntilts * boxsize_ht * boxsize_ht [mask], 3).</span>
<span class="sd">        :return: Positionally encoded spatial coordinates</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_type</span> <span class="o">==</span> <span class="s1">&#39;gaussian&#39;</span><span class="p">:</span>
            <span class="c1"># expand freqs with singleton dimension along the batch dimensions, e.g. dim (1, ..., 1, n_feats, 3)</span>
            <span class="n">freqs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">freqs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># 1 x 1 x 3*D2 x 3</span>
            <span class="c1"># calculate features as torch.dot(k, freqs): compute x,y,z components of k then sum x,y,z components</span>
            <span class="n">kxkykz</span> <span class="o">=</span> <span class="n">coords</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">freqs</span>  <span class="c1"># B x N*D*D[mask] x 3*D2 x 3</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">kxkykz</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># B x N*D*D[mask] x 3*D2</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># B x N*D*D[mask] x 3*D2</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># B x N*D*D[mask] x 3*D2</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">s</span><span class="p">,</span> <span class="n">c</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># B x N*D*D[mask] x 3*D == B x N*D*D[mask] x in_dim - zdim</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># expand freqs with singleton dimension along the batch dimensions, e.g. dim (1, ..., 1, n_feats)</span>
            <span class="n">freqs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">freqs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 1 x 1 x 1 x D2</span>
            <span class="c1"># calculate the features as freqs scaled by coords</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">coords</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">freqs</span>  <span class="c1"># B x N*D*D[mask] x 3 x D2</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># B x N*D*D[mask] x 3 x D2</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># B x N*D*D[mask] x 3 x D2</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">s</span><span class="p">,</span> <span class="n">c</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># B x N*D*D[mask] x 3 x D</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">coords</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">zdim</span><span class="p">)</span>  <span class="c1"># B x N*D*D[mask] x in_dim-zdim</span>

        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="FTPositionalDecoder.cat_z">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.FTPositionalDecoder.html#tomodrgn.models.FTPositionalDecoder.cat_z">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">cat_z</span><span class="p">(</span><span class="n">coords</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
              <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Concatenate each 3-D spatial coordinate (from a particular particle&#39;s particular tilt image) with the latent embedding assigned to that particle.</span>

<span class="sd">        :param coords: 3-D spatial frequency coordinates at which to decode corresponding voxel intensity, possibly NestedTensor, shape (batch, ntilts * boxsize**2 [mask], self.in_dim - zdim)</span>
<span class="sd">        :param z: latent embedding for each particle, shape (batch, zdim)</span>
<span class="sd">        :return: concatenated coordinates and latent embedding tensors, possibly NestedTensor, shape (batch, ntilts * boxsize**2 [mask], self.in_dim)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># confirm coords and z have same batch size (axis over which they will eventualy be aligned)</span>
        <span class="k">if</span> <span class="n">z</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># coords should have batch dimension populated from positional_encoding, but z might not</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">coords</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">z</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>

        <span class="c1"># repeat z along a new axis corresponding to spatial frequency coordinates for each particle&#39;s images</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">repeat</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="s1">&#39;batch zdim -&gt; batch repeat_ntilts_npixels zdim&#39;</span><span class="p">,</span> <span class="n">repeat_ntilts_npixels</span><span class="o">=</span><span class="n">coords</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># concatenate coords with z along the last axis (coordinate + zdim value)</span>
        <span class="n">coords_z</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pack</span><span class="p">([</span><span class="n">coords</span><span class="p">,</span> <span class="n">z</span><span class="p">],</span> <span class="s1">&#39;batch ntilts_npixels *&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">coords_z</span></div>


<div class="viewcode-block" id="FTPositionalDecoder.forward">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.FTPositionalDecoder.html#tomodrgn.models.FTPositionalDecoder.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">coords</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Decode a batch of lattice coordinates concatenated with the corresponding latent embedding to Hartley Transform spatial frequency amplitudes.</span>

<span class="sd">        :param coords: masked 3-D spatial frequency coordinates (e.g. from Lattice.coords), shape (batch, ntilts * boxsize_ht * boxsize_ht [mask], 3)</span>
<span class="sd">        :param z: latent embedding per-particle, shape (batch, zdim)</span>
<span class="sd">        :return: Decoded voxel intensities at the specified 3-D spatial frequencies.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># evaluate model on all pixel coordinates for each image</span>
        <span class="c1"># TODO revisit whether it is worth leveraging Hermitian symmetry to only evaluate half of each image</span>
        <span class="n">full_image</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">coords</span><span class="o">=</span><span class="n">coords</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">)</span>

        <span class="c1"># return hartley information (FT real - FT imag) for each image</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">full_image</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">full_image</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">image</span></div>


<div class="viewcode-block" id="FTPositionalDecoder.decode">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.FTPositionalDecoder.html#tomodrgn.models.FTPositionalDecoder.decode">[docs]</a>
    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">coords</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
               <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Decode a batch of lattice coordinates concatenated with the corresponding latent embedding to Fourier Transform spatial frequency amplitudes.</span>

<span class="sd">        :param coords: 3-D spatial frequency coordinates (e.g. from Lattice.coords) concatenated with the shape (batch, ntilts * boxsize_ht * boxsize_ht [mask], 3).</span>
<span class="sd">        :param z: latent embedding per-particle, shape (batch, zdim)</span>
<span class="sd">        :return: Decoded voxel intensities at the specified 3-D spatial frequencies.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># sanity check inputs coordinates are within range (-0.5, 0.5)</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">coords</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">&lt;</span> <span class="mf">1e-4</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="sa">f</span><span class="s1">&#39;coords.max(): </span><span class="si">{</span><span class="n">coords</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">}</span><span class="s1">; coords.min(): </span><span class="si">{</span><span class="n">coords</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span>

        <span class="c1"># TODO reimplement hermetian symmetry to evaluate +z coords only?</span>

        <span class="c1"># positionally encode x,y,z 3-D coordinate</span>
        <span class="n">coords_pe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">(</span><span class="n">coords</span><span class="p">)</span>

        <span class="c1"># concatenate each spatial coordinate with the appropriate latent embedding to be conditioned on</span>
        <span class="k">if</span> <span class="n">z</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">coords_pe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cat_z</span><span class="p">(</span><span class="n">coords_pe</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>

        <span class="c1"># evaluate the model</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">coords_pe</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">result</span></div>


<div class="viewcode-block" id="FTPositionalDecoder.eval_volume_batch">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.FTPositionalDecoder.html#tomodrgn.models.FTPositionalDecoder.eval_volume_batch">[docs]</a>
    <span class="k">def</span> <span class="nf">eval_volume_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                          <span class="n">coords</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
                          <span class="n">extent</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate the model on 3-D volume coordinates given an optional (batch of) latent coordinate.</span>

<span class="sd">        :param coords: lattice coords on the x-y plane, shape (boxsize_ht**2, 3)</span>
<span class="sd">        :param z: latent embedding associated with the volume to decode, shape (batchsize, zdim)</span>
<span class="sd">        :param extent: maximum value of the grid along each dimension, typically &lt;= 0.5 to constrain points to range (-0.5, 0.5)</span>
<span class="sd">        :return: batch of decoded volumes directly output from the model (Fourier space, symmetrized) with no postprocessing applied, shape (batchsize, boxsize_ht, boxsize_ht, boxsize_ht).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># sanity check inputs</span>
        <span class="k">assert</span> <span class="n">extent</span> <span class="o">&lt;=</span> <span class="mf">0.5</span>
        <span class="k">if</span> <span class="n">z</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">z</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>

        <span class="c1"># get key array sizes</span>
        <span class="n">batchsize</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="k">if</span> <span class="n">z</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="n">boxsize_ht</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">coords</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>

        <span class="c1"># preallocate array to store batch of decoded volumes</span>
        <span class="n">batch_vol_f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batchsize</span><span class="p">,</span> <span class="n">boxsize_ht</span><span class="p">,</span> <span class="n">boxsize_ht</span><span class="p">,</span> <span class="n">boxsize_ht</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">coords</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">coords</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># evaluate the batch of volumes slice-by-slice along z axis to avoid potential memory overflows from evaluating D**3 voxels at once</span>
        <span class="n">zslices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">extent</span><span class="p">,</span> <span class="n">extent</span><span class="p">,</span> <span class="n">boxsize_ht</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">batch_vol_f</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">zslice</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">zslices</span><span class="p">):</span>
            <span class="c1"># create the z slice to evaluate</span>
            <span class="n">xy_coords</span> <span class="o">=</span> <span class="n">coords</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">zslice</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">coords</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">coords</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># only evaluate coords within `extent` radius (if extent==0.5, nyquist limit in reciprocal space)</span>
            <span class="n">slice_mask</span> <span class="o">=</span> <span class="n">xy_coords</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">extent</span> <span class="o">**</span> <span class="mi">2</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">slice_mask</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># no coords left to evaluate after masking, skip to next zslice in next loop iteration</span>
                <span class="k">continue</span>
            <span class="n">xy_coords</span> <span class="o">=</span> <span class="n">xy_coords</span><span class="p">[</span><span class="n">slice_mask</span><span class="p">]</span>

            <span class="c1"># expand coords to batch size</span>
            <span class="n">batch_xy_coords</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">batchsize</span> <span class="o">*</span> <span class="p">[</span><span class="n">xy_coords</span><span class="p">])</span>

            <span class="c1"># reconstruct the slice by passing it through the model</span>
            <span class="n">batch_xy_slice</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">coords</span><span class="o">=</span><span class="n">batch_xy_coords</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">)</span>

            <span class="c1"># fill the corresponding slice in the 3-D volume with these amplitudes</span>
            <span class="n">batch_vol_f</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="n">slice_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">boxsize_ht</span><span class="p">,</span> <span class="n">boxsize_ht</span><span class="p">)]</span> <span class="o">=</span> <span class="n">batch_xy_slice</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">batch_vol_f</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">batch_vol_f</span></div>


<div class="viewcode-block" id="FTPositionalDecoder.postprocess_volume_batch">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.FTPositionalDecoder.html#tomodrgn.models.FTPositionalDecoder.postprocess_volume_batch">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">postprocess_volume_batch</span><span class="p">(</span><span class="n">batch_vols</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">norm</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
                                 <span class="n">iht_downsample_scaling_correction</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                                 <span class="n">lowpass_mask</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                                 <span class="n">flip</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                                 <span class="n">invert</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply post-volume-decoding processing steps: downsampling scaling correction, lowpass filtering, inverse fourier transform, volume handedness flipping, volume data sign inversion.</span>

<span class="sd">        :param batch_vols: batch of fourier space non-symmetrized volumes directly from eval_vol_batch, shape (nvols, boxsize, boxsize, boxsize)</span>
<span class="sd">        :param norm: tuple of floats representing mean and standard deviation of preprocessed particles used during model training</span>
<span class="sd">        :param iht_downsample_scaling_correction: a global scaling factor applied when forward and inverse fourier / hartley transforming.</span>
<span class="sd">                This is calculated and applied internally by the fft.py module as a function of array shape.</span>
<span class="sd">                Thus, when the volume is downsampled, a further correction is required.</span>
<span class="sd">        :param lowpass_mask: a binary mask applied to fourier space symmetrized volumes to low pass filter the reconstructions.</span>
<span class="sd">                Typically, the same mask is used for all volumes via broadcasting, thus this may be of shape (1, boxsize, boxsize, boxsize) or (nvols, boxsize, boxsize, boxsize).</span>
<span class="sd">        :param flip: Whether to invert the volume chirality by flipping the data order along the z axis.</span>
<span class="sd">        :param invert: Whether to invert the data light-on-dark vs dark-on-light convention, relative to the reconstruction returned by the decoder module.</span>
<span class="sd">        :return: Postprocessed volume batch in real space</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># sanity check inputs</span>
        <span class="k">assert</span> <span class="n">batch_vols</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;The volume batch must have four dimensions (batch size, boxsize, boxsize, boxsize). Found </span><span class="si">{</span><span class="n">batch_vols</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="n">lowpass_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">batch_vols</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span> <span class="o">==</span> <span class="n">lowpass_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:],</span> <span class="sa">f</span><span class="s1">&#39;The volume batch must have the same volume dimensions as the lowpass mask. Found </span><span class="si">{</span><span class="n">batch_vols</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">lowpass_mask</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span>

        <span class="c1"># convert torch tensor to numpy array for future operations</span>
        <span class="n">batch_vols</span> <span class="o">=</span> <span class="n">batch_vols</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="c1"># normalize the volume (mean and standard deviation) by normalization used when training the model</span>
        <span class="n">batch_vols</span> <span class="o">=</span> <span class="n">batch_vols</span> <span class="o">*</span> <span class="n">norm</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">norm</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># lowpass filter with fourier space mask</span>
        <span class="k">if</span> <span class="n">lowpass_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_vols</span> <span class="o">=</span> <span class="n">batch_vols</span> <span class="o">*</span> <span class="n">lowpass_mask</span>

        <span class="c1"># transform to real space and scale values if downsampling was applied</span>
        <span class="n">batch_vols</span> <span class="o">=</span> <span class="n">fft</span><span class="o">.</span><span class="n">iht3_center</span><span class="p">(</span><span class="n">batch_vols</span><span class="p">)</span>
        <span class="n">batch_vols</span> <span class="o">*=</span> <span class="n">iht_downsample_scaling_correction</span>

        <span class="k">if</span> <span class="n">flip</span><span class="p">:</span>
            <span class="n">batch_vols</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">batch_vols</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">invert</span><span class="p">:</span>
            <span class="n">batch_vols</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="k">return</span> <span class="n">batch_vols</span></div>
</div>



<div class="viewcode-block" id="TiltSeriesEncoder">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.TiltSeriesEncoder.html#tomodrgn.models.TiltSeriesEncoder">[docs]</a>
<span class="k">class</span> <span class="nc">TiltSeriesEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A module to encode multiple (tilt) images of a particle to a latent embedding.</span>
<span class="sd">    The module comprises two submodules: `encoder_a` and `encoder_b`.</span>
<span class="sd">    Encoder_a is a ResidLinearMLP module that embeds a single tilt image of a particle with `in_dim` features (pixels) to an embedding with `out_dim_a` features.</span>
<span class="sd">    Encoder_b is a ResidLinearMLP module that pools all per-tilt-image embeddings from `encoder_a` by a `pooling_function`,</span>
<span class="sd">    and embeds this pooled representation to a single latent embedding with `out_dim` features.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">hidden_layers_a</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                 <span class="n">hidden_dim_a</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
                 <span class="n">out_dim_a</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
                 <span class="n">hidden_layers_b</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                 <span class="n">hidden_dim_b</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
                 <span class="n">out_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">activation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
                 <span class="n">ntilts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">41</span><span class="p">,</span>
                 <span class="n">pooling_function</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;concatenate&#39;</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;median&#39;</span><span class="p">,</span> <span class="s1">&#39;set_encoder&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;concatenate&#39;</span><span class="p">,</span>
                 <span class="n">num_seeds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
                 <span class="n">layer_norm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create the TiltSeriesEncoder module.</span>

<span class="sd">        :param in_dim: number of input features to the module (typically the number of pixels in a masked image)</span>
<span class="sd">        :param hidden_layers_a: number of intermediate hidden layers in the encoder_a submodule</span>
<span class="sd">        :param hidden_dim_a: number of features in each hidden layer in the encoder_a submodule</span>
<span class="sd">        :param out_dim_a: number of output features from the encoder_a submodule (sometimes referred to as the intermediate latent embedding dimensionality)</span>
<span class="sd">        :param hidden_layers_b: number of intermediate hidden layers in the encoder_b submodule</span>
<span class="sd">        :param hidden_dim_b: number of features in each hidden layer in the encoder_b submodule</span>
<span class="sd">        :param out_dim: number of output features from the encoder_b submodule (sometimes referred to as the latent embedding dimensionality or zdim)</span>
<span class="sd">        :param activation: activation function to be applied after each layer, either `torch.nn.ReLU` or `torch.nn.LeakyReLU`</span>
<span class="sd">        :param ntilts: the number of tilt images per particle, used in setting the number of input features to `encoder_b`</span>
<span class="sd">        :param pooling_function: the method used to pool the per-tilt-image intermediate latent representations prior to `encoder_b`</span>
<span class="sd">        :param num_seeds: number of seed vectors to use in the set encoder module. Generally should be set to 1</span>
<span class="sd">        :param num_heads: number of heads for multihead attention in the set encoder module. Generally should be set to a power of 2</span>
<span class="sd">        :param layer_norm: whether to apply layer normalization in the set encoder module</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># initialize the parent nn.Module class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># assign attributes from parameters used at instance creation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span> <span class="o">=</span> <span class="n">in_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layers_a</span> <span class="o">=</span> <span class="n">hidden_layers_a</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim_a</span> <span class="o">=</span> <span class="n">hidden_dim_a</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim_a</span> <span class="o">=</span> <span class="n">out_dim_a</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layers_b</span> <span class="o">=</span> <span class="n">hidden_layers_b</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim_b</span> <span class="o">=</span> <span class="n">hidden_dim_b</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ntilts</span> <span class="o">=</span> <span class="n">ntilts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooling_function</span> <span class="o">=</span> <span class="n">pooling_function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_seeds</span> <span class="o">=</span> <span class="n">num_seeds</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">layer_norm</span>

        <span class="k">assert</span> <span class="n">hidden_layers_a</span> <span class="o">&gt;=</span> <span class="mi">0</span>  <span class="c1"># possible to have no hidden layers, just a direct mapping from input to output</span>
        <span class="k">assert</span> <span class="n">hidden_layers_b</span> <span class="o">&gt;=</span> <span class="mi">0</span>  <span class="c1"># possible to have no hidden layers, just a direct mapping from input to output</span>
        <span class="k">if</span> <span class="n">pooling_function</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;concatenate&#39;</span><span class="p">,</span> <span class="s1">&#39;set_encoder&#39;</span><span class="p">]:</span>
            <span class="c1"># other pooling functions do not use ntilts so not relevant</span>
            <span class="k">assert</span> <span class="n">ntilts</span> <span class="o">&gt;</span> <span class="mi">1</span>  <span class="c1"># having ntilts == 1 is very likely to cause problems with squeezing and broadcasting tensors</span>

        <span class="c1"># encoder1 encodes each identically-masked tilt image, independently</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_a</span> <span class="o">=</span> <span class="n">ResidLinearMLP</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_layers_a</span><span class="p">,</span> <span class="n">hidden_dim_a</span><span class="p">,</span> <span class="n">out_dim_a</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span>

        <span class="c1"># encoder2 merges ntilts-concatenated encoder1 information and encodes further to latent space via one of</span>
        <span class="c1"># (&#39;concatenate&#39;, &#39;max&#39;, &#39;mean&#39;, &#39;median&#39;, &#39;set_encoder&#39;)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_function</span> <span class="o">==</span> <span class="s1">&#39;concatenate&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pooling_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Rearrange</span><span class="p">(</span><span class="s1">&#39;batch tilt pixels -&gt; batch (tilt pixels)&#39;</span><span class="p">),</span>
                                               <span class="n">activation</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder_b</span> <span class="o">=</span> <span class="n">ResidLinearMLP</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="n">out_dim_a</span> <span class="o">*</span> <span class="n">ntilts</span><span class="p">,</span>
                                            <span class="n">nlayers</span><span class="o">=</span><span class="n">hidden_layers_b</span><span class="p">,</span>
                                            <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim_b</span><span class="p">,</span>
                                            <span class="n">out_dim</span><span class="o">=</span><span class="n">out_dim</span><span class="p">,</span>
                                            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_function</span> <span class="o">==</span> <span class="s1">&#39;max&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pooling_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Reduce</span><span class="p">(</span><span class="s1">&#39;batch tilt pixels -&gt; batch pixels&#39;</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">),</span>
                                               <span class="n">activation</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder_b</span> <span class="o">=</span> <span class="n">ResidLinearMLP</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="n">out_dim_a</span><span class="p">,</span>
                                            <span class="n">nlayers</span><span class="o">=</span><span class="n">hidden_layers_b</span><span class="p">,</span>
                                            <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim_b</span><span class="p">,</span>
                                            <span class="n">out_dim</span><span class="o">=</span><span class="n">out_dim</span><span class="p">,</span>
                                            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_function</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pooling_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Reduce</span><span class="p">(</span><span class="s1">&#39;batch tilt pixels -&gt; batch pixels&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">),</span>
                                               <span class="n">activation</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder_b</span> <span class="o">=</span> <span class="n">ResidLinearMLP</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="n">out_dim_a</span><span class="p">,</span>
                                            <span class="n">nlayers</span><span class="o">=</span><span class="n">hidden_layers_b</span><span class="p">,</span>
                                            <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim_b</span><span class="p">,</span>
                                            <span class="n">out_dim</span><span class="o">=</span><span class="n">out_dim</span><span class="p">,</span>
                                            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_function</span> <span class="o">==</span> <span class="s1">&#39;median&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pooling_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">MedianPool1d</span><span class="p">(</span><span class="n">pooling_axis</span><span class="o">=-</span><span class="mi">2</span><span class="p">),</span>
                                               <span class="n">activation</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder_b</span> <span class="o">=</span> <span class="n">ResidLinearMLP</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="n">out_dim_a</span><span class="p">,</span>
                                            <span class="n">nlayers</span><span class="o">=</span><span class="n">hidden_layers_b</span><span class="p">,</span>
                                            <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim_b</span><span class="p">,</span>
                                            <span class="n">out_dim</span><span class="o">=</span><span class="n">out_dim</span><span class="p">,</span>
                                            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_function</span> <span class="o">==</span> <span class="s1">&#39;set_encoder&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pooling_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">set_transformer</span><span class="o">.</span><span class="n">SetTransformer</span><span class="p">(</span><span class="n">dim_input</span><span class="o">=</span><span class="n">out_dim_a</span><span class="p">,</span>
                                                                              <span class="n">num_outputs</span><span class="o">=</span><span class="n">num_seeds</span><span class="p">,</span>
                                                                              <span class="n">dim_output</span><span class="o">=</span><span class="n">out_dim</span><span class="p">,</span>
                                                                              <span class="n">dim_hidden</span><span class="o">=</span><span class="n">hidden_dim_b</span><span class="p">,</span>
                                                                              <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                                              <span class="n">ln</span><span class="o">=</span><span class="n">layer_norm</span><span class="p">),</span>
                                               <span class="n">Rearrange</span><span class="p">(</span><span class="s1">&#39;batch 1 latent -&gt; batch latent&#39;</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder_b</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span>

<div class="viewcode-block" id="TiltSeriesEncoder.forward">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.TiltSeriesEncoder.html#tomodrgn.models.TiltSeriesEncoder.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pass data forward through the module.</span>

<span class="sd">        :param batch: Input data tensor, shape (batch, ntilts, boxsize*boxsize[enc_mask])</span>
<span class="sd">        :return: Output data tensor, shape (batch, zdim*2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># pass each tilt independently through encoder_a (parallelizing along both batch and ntilts dimensions)</span>
        <span class="n">batch_tilts_intermediate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_a</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="c1"># pool each tilt&#39;s intermediate latent embedding along the ntilts dimension</span>
        <span class="n">batch_tilts_pooled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_layer</span><span class="p">(</span><span class="n">batch_tilts_intermediate</span><span class="p">)</span>
        <span class="c1"># obtain parameters for per-particle latent embedding mean and log(variance) as a single concatenated tensor output</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_b</span><span class="p">(</span><span class="n">batch_tilts_pooled</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">z</span></div>


<div class="viewcode-block" id="TiltSeriesEncoder.reparameterize">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.TiltSeriesEncoder.html#tomodrgn.models.TiltSeriesEncoder.reparameterize">[docs]</a>
    <span class="k">def</span> <span class="nf">reparameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                       <span class="n">mu</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">logvar</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reparamaterization trick to allow backpropogation through semi-randomly sampled latent embedding.</span>
<span class="sd">        Sampling a latent embedding from a gaussian parameterized by mu and logvar is an operation without an associated gradient with respect to inputs, and therefore breaks training.</span>
<span class="sd">        We reparamaterize such that the latent embedding is deterministically calculated as `z = epsilon * standard_deviation + mean`.</span>
<span class="sd">        This representation &quot;outsources&quot; the randomness from `z` itself to `epsilon`, and allows gradient calculation through `z` to `standard_deviation` and `mean`, and onward to earlier layers.</span>

<span class="sd">        :param mu: mean parameterizing latent embeddings `z`, shape (batch, zdim)</span>
<span class="sd">        :param logvar: log(variance) parameterizing latent embeddings `z`, shape (batch, zdim)</span>
<span class="sd">        :return: reparameterized latent embeddings `z`, shape (batch, zdim)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># no need for reparameterization at inference time; better to be deterministic</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mu</span>

        <span class="c1"># otherwise perform reparameterization</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">std</span> <span class="o">+</span> <span class="n">mu</span></div>
</div>



<div class="viewcode-block" id="ResidLinearMLP">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.ResidLinearMLP.html#tomodrgn.models.ResidLinearMLP">[docs]</a>
<span class="k">class</span> <span class="nc">ResidLinearMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiple connected Residual Blocks as a Multi-Layer Perceptron.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">nlayers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">out_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">activation</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create the ResidLinearMLP module.</span>

<span class="sd">        :param in_dim: number of input features to the module</span>
<span class="sd">        :param nlayers: number of intermediate hidden layers in the module</span>
<span class="sd">        :param hidden_dim: number of features in each hidden layer</span>
<span class="sd">        :param out_dim: number of output features from the module</span>
<span class="sd">        :param activation: activation function to be applied after each layer, either `torch.nn.ReLU` or `torch.nn.LeakyReLU`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># intialize the parent nn.Module class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># create the first layer to receive input</span>
        <span class="c1"># the layer will be a ResidLinear layer if possible, otherwise a Linear layer</span>
        <span class="k">if</span> <span class="n">in_dim</span> <span class="o">==</span> <span class="n">hidden_dim</span><span class="p">:</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">ResidLinear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">())</span>

        <span class="c1"># append the hidden layers to the module as ResidLinear layers</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nlayers</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidLinear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">())</span>

        <span class="c1"># append the output layer to the module without a final activation function</span>
        <span class="c1"># the layer will be a ResidLinear layer if possible, otherwise a Linear layer</span>
        <span class="k">if</span> <span class="n">out_dim</span> <span class="o">==</span> <span class="n">hidden_dim</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidLinear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">))</span>

        <span class="c1"># create the overall module as a sequential pass through the defined layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">main</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

<div class="viewcode-block" id="ResidLinearMLP.forward">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.ResidLinearMLP.html#tomodrgn.models.ResidLinearMLP.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pass data forward through the module.</span>

<span class="sd">        :param x: Input data tensor.</span>
<span class="sd">        :return: Output data tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="ResidLinear">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.ResidLinear.html#tomodrgn.models.ResidLinear">[docs]</a>
<span class="k">class</span> <span class="nc">ResidLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Residual Block layer consisting of a single linear layer with an identity skip connection.</span>
<span class="sd">    Note that the identity mapping requires that the number of input and output features are the same for element-wise addition.</span>
<span class="sd">    References: https://arxiv.org/abs/1512.03385</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">nin</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">nout</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create the ResidLinear layer.</span>

<span class="sd">        :param nin: number of input features to the linear layer</span>
<span class="sd">        :param nout: number of output features to the linear layer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nin</span><span class="p">,</span> <span class="n">nout</span><span class="p">)</span>

<div class="viewcode-block" id="ResidLinear.forward">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.ResidLinear.html#tomodrgn.models.ResidLinear.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pass data forward through the layer.</span>

<span class="sd">        :param x: Input data tensor.</span>
<span class="sd">        :return: Output data tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># output is element-wise addition of the output of the linear layer given x with (identity-mapped) input x</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">z</span></div>
</div>



<div class="viewcode-block" id="MedianPool1d">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.MedianPool1d.html#tomodrgn.models.MedianPool1d">[docs]</a>
<span class="k">class</span> <span class="nc">MedianPool1d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Median pool module.</span>
<span class="sd">    Primarily exists due to limitations in pre-existing layer-based definitions of median.</span>
<span class="sd">     * torch.nn does not have a MedianPool module</span>
<span class="sd">     * einops does not support a callable (such as `torch.median`) when defining a Reduce layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pooling_axis</span><span class="o">=-</span><span class="mi">2</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create the MedianPool1d layer.</span>

<span class="sd">        :param pooling_axis: the tensor axis over which to take the median</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooling_axis</span> <span class="o">=</span> <span class="n">pooling_axis</span>

<div class="viewcode-block" id="MedianPool1d.forward">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.MedianPool1d.html#tomodrgn.models.MedianPool1d.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pass data forward through the layer.</span>

<span class="sd">        :param x: Input data tensor.</span>
<span class="sd">        :return: Output data tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">autocast</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>  <span class="c1"># torch.quantile and torch.median do not support fp16 so casting to fp32 in case AMP is used</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pooling_axis</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">x</span></div>
</div>



<div class="viewcode-block" id="DataParallelPassthrough">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.DataParallelPassthrough.html#tomodrgn.models.DataParallelPassthrough">[docs]</a>
<span class="k">class</span> <span class="nc">DataParallelPassthrough</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class to wrap underlying module in DataParallel for GPU-parallelized computations, but allow accessing underlying module attributes and methods.</span>
<span class="sd">    Intended use: `model = DataParallelPassthrough(model)`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the requested attribute or method from the parent DataParallel module if it exists, otherwise from the wrapped module.</span>

<span class="sd">        :param name: name of the attribute or method to request</span>
<span class="sd">        :return: the requested attribute or method</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__getattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>



<div class="viewcode-block" id="VolumeGenerator">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.VolumeGenerator.html#tomodrgn.models.VolumeGenerator">[docs]</a>
<span class="k">class</span> <span class="nc">VolumeGenerator</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convenience class to generate volume(s) from a trained tomoDRGN model.</span>
<span class="sd">    Supports evaluating homogeneous (`train_nn`) or heterogeneous (`train_vae`) models.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">config</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">],</span>
                 <span class="n">weights_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">model</span><span class="p">:</span> <span class="n">TiltSeriesHetOnlyVAE</span> <span class="o">|</span> <span class="n">FTPositionalDecoder</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">lat</span><span class="p">:</span> <span class="n">lattice</span><span class="o">.</span><span class="n">Lattice</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">amp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Instantiate a `VolumeGenerator` object.</span>
<span class="sd">        :param config: path to trained model `config.pkl` from `train_vae.py` or `train_nn.py`, or the corresponding preloaded config dictionary</span>
<span class="sd">        :param weights_path: path to trained model `weights.*.pkl` from `train_vae.py` or `train_nn.py`.</span>
<span class="sd">                Prefer specifying `weights_path` over `model` when a model is not yet loaded in memory.</span>
<span class="sd">        :param model: preloaded model from which to evaluate volumes.</span>
<span class="sd">                Prefer specifying `model` over `weights_path` when a model is already loaded in memory. Must specify `lat` with `model`.</span>
<span class="sd">        :param lat: preloaded tomodrgn lattice object. Must specify `model` with `lat`.</span>
<span class="sd">        :param amp: Enable or disable use of mixed-precision model inference</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># set the device</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_default_device</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">):</span>
            <span class="n">amp</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">log</span><span class="p">(</span><span class="s1">&#39;Warning: pytorch AMP does not support non-CUDA (e.g. cpu) devices. Automatically disabling AMP and continuing&#39;</span><span class="p">)</span>
            <span class="c1"># https://github.com/pytorch/pytorch/issues/55374</span>

        <span class="c1"># load the model configuration</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">str</span><span class="p">:</span>
            <span class="n">cfg</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">load_pkl</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">dict</span>
            <span class="n">cfg</span> <span class="o">=</span> <span class="n">config</span>

        <span class="c1"># set parameters for volume generation</span>
        <span class="n">boxsize_ht</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;lattice_args&#39;</span><span class="p">][</span><span class="s1">&#39;boxsize&#39;</span><span class="p">])</span>  <span class="c1"># image size + 1</span>
        <span class="n">zdim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">][</span><span class="s1">&#39;zdim&#39;</span><span class="p">])</span> <span class="k">if</span> <span class="s1">&#39;zdim&#39;</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;model_args&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">norm</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;dataset_args&#39;</span><span class="p">][</span><span class="s1">&#39;norm&#39;</span><span class="p">]</span>
        <span class="n">angpix</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;angpix&#39;</span><span class="p">])</span>

        <span class="c1"># load the model and lattice</span>
        <span class="k">if</span> <span class="n">weights_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Cannot specify both weights_path` and `model`&#39;</span>
            <span class="k">if</span> <span class="n">zdim</span><span class="p">:</span>
                <span class="c1"># load a VAE model</span>
                <span class="n">model</span><span class="p">,</span> <span class="n">lat</span> <span class="o">=</span> <span class="n">TiltSeriesHetOnlyVAE</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">cfg</span><span class="p">,</span>
                                                       <span class="n">weights</span><span class="o">=</span><span class="n">weights_path</span><span class="p">,</span>
                                                       <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># load a non-VAE decoder-only model</span>
                <span class="n">model</span><span class="p">,</span> <span class="n">lat</span> <span class="o">=</span> <span class="n">FTPositionalDecoder</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">cfg</span><span class="p">,</span>
                                                      <span class="n">weights</span><span class="o">=</span><span class="n">weights_path</span><span class="p">,</span>
                                                      <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># using a pre-loaded model and lattice</span>
            <span class="k">assert</span> <span class="n">weights_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Cannot specify both weights_path` and `model`&#39;</span>
            <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="n">TiltSeriesHetOnlyVAE</span><span class="p">,</span> <span class="n">FTPositionalDecoder</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;Unrecognized model type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Must specify one of weights_path` or `model`&#39;</span><span class="p">)</span>

        <span class="c1"># define the volume evaluation and postprocessing functions associated with the model</span>
        <span class="k">if</span> <span class="n">zdim</span><span class="p">:</span>
            <span class="n">eval_volume_batch</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">eval_volume_batch</span>
            <span class="n">postprocess_volume_batch</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">postprocess_volume_batch</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_volume_batch</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval_volume_batch</span>
            <span class="n">postprocess_volume_batch</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">postprocess_volume_batch</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_boxsize_ht</span> <span class="o">=</span> <span class="n">boxsize_ht</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_angpix</span> <span class="o">=</span> <span class="n">angpix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zdim</span> <span class="o">=</span> <span class="n">zdim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lat</span> <span class="o">=</span> <span class="n">lat</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">amp</span> <span class="o">=</span> <span class="n">amp</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_volume_batch</span> <span class="o">=</span> <span class="n">eval_volume_batch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">postprocess_volume_batch</span> <span class="o">=</span> <span class="n">postprocess_volume_batch</span>

<div class="viewcode-block" id="VolumeGenerator.generate_volumes">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.VolumeGenerator.html#tomodrgn.models.VolumeGenerator.generate_volumes">[docs]</a>
    <span class="k">def</span> <span class="nf">generate_volumes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                         <span class="n">z</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
                         <span class="n">out_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                         <span class="n">out_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;vol&#39;</span><span class="p">,</span>
                         <span class="n">downsample</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                         <span class="n">lowpass</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                         <span class="n">flip</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                         <span class="n">invert</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                         <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate volumes at specified latent embeddings and save to specified output directory.</span>
<span class="sd">        Generated volume filename format is `out_dir / out_name _ {i:03d if multiple volumes} .mrc`</span>

<span class="sd">        :param z: latent embeddings to evaluate as numpy array of shape `(nptcls, zdim)`, or path to a .txt or .pkl file containing array of shape (nptcls, zdim).</span>
<span class="sd">                `None` if evaluating a homogeneous tomodrgn model.</span>
<span class="sd">        :param out_dir: path to output directory in which to save output mrc file(s)</span>
<span class="sd">        :param out_name: string to prepend to output .mrc file name(s)</span>
<span class="sd">        :param downsample: downsample reconstructed volumes to this box size (units: px) by Fourier cropping, None means to skip downsampling</span>
<span class="sd">        :param lowpass: lowpass filter reconstructed volumes to this resolution (units: Å), None means to skip lowpass filtering</span>
<span class="sd">        :param flip: flip the chirality of the reconstructed volumes by inverting along the z axis</span>
<span class="sd">        :param invert: invert the data sign of the reconstructed volumes (light-on-dark vs dark-on-light)</span>
<span class="sd">        :param batch_size: batch size to parallelize volume generation (32-64 works well for box64 volumes)</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># sanity check inputs for evaluating model and postprocessing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_inputs</span><span class="p">(</span><span class="n">downsample</span><span class="o">=</span><span class="n">downsample</span><span class="p">,</span>
                           <span class="n">out_dir</span><span class="o">=</span><span class="n">out_dir</span><span class="p">)</span>

        <span class="c1"># prepare inputs for evaluating model and postprocessing</span>
        <span class="n">coords</span><span class="p">,</span> <span class="n">extent</span><span class="p">,</span> <span class="n">iht_downsample_scaling_correction</span><span class="p">,</span> <span class="n">angpix</span><span class="p">,</span> <span class="n">lowpass_mask</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_inputs</span><span class="p">(</span><span class="n">out_dir</span><span class="o">=</span><span class="n">out_dir</span><span class="p">,</span>
                                                                                                          <span class="n">downsample</span><span class="o">=</span><span class="n">downsample</span><span class="p">,</span>
                                                                                                          <span class="n">lowpass</span><span class="o">=</span><span class="n">lowpass</span><span class="p">,</span>
                                                                                                          <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">)</span>

        <span class="c1"># set context managers and flags for inference mode evaluation loop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">amp</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">z</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">batch_vols</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_volume_batch</span><span class="p">(</span><span class="n">coords</span><span class="o">=</span><span class="n">coords</span><span class="p">,</span>
                                                        <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">,</span>
                                                        <span class="n">extent</span><span class="o">=</span><span class="n">extent</span><span class="p">)</span>
                    <span class="n">batch_vols</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">postprocess_volume_batch</span><span class="p">(</span><span class="n">batch_vols</span><span class="o">=</span><span class="n">batch_vols</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># exclude symmetrized +k frequency</span>
                                                               <span class="n">norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span>
                                                               <span class="n">iht_downsample_scaling_correction</span><span class="o">=</span><span class="n">iht_downsample_scaling_correction</span><span class="p">,</span>
                                                               <span class="n">lowpass_mask</span><span class="o">=</span><span class="n">lowpass_mask</span><span class="p">,</span>
                                                               <span class="n">flip</span><span class="o">=</span><span class="n">flip</span><span class="p">,</span>
                                                               <span class="n">invert</span><span class="o">=</span><span class="n">invert</span><span class="p">)</span>
                    <span class="n">out_mrc</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">out_dir</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">out_name</span><span class="si">}</span><span class="s1">.mrc&#39;</span>
                    <span class="n">mrc</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="n">out_mrc</span><span class="p">,</span>
                              <span class="n">array</span><span class="o">=</span><span class="n">batch_vols</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                              <span class="n">angpix</span><span class="o">=</span><span class="n">angpix</span><span class="p">)</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># batch size cannot be larger than the number of latent embeddings to evaluate, or is 1 if homogeneous model</span>
                    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="c1"># construct z iterator</span>
                    <span class="n">z_iterator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">split_size_or_sections</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                    <span class="n">num_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">z_iterator</span><span class="p">)</span>
                    <span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Generating </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="si">}</span><span class="s1"> volumes in batches of </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

                    <span class="c1"># prepare threadpool for parallelized file writing</span>
                    <span class="k">with</span> <span class="n">Pool</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">(),</span> <span class="n">batch_size</span><span class="p">))</span> <span class="k">as</span> <span class="n">p</span><span class="p">:</span>
                        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">z_batch</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">z_iterator</span><span class="p">):</span>
                            <span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;    Generating volume batch </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1"> / </span><span class="si">{</span><span class="n">num_batches</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
                            <span class="n">z_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">z_batch</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                            <span class="n">batch_vols</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_volume_batch</span><span class="p">(</span><span class="n">coords</span><span class="o">=</span><span class="n">coords</span><span class="p">,</span>
                                                                <span class="n">z</span><span class="o">=</span><span class="n">z_batch</span><span class="p">,</span>
                                                                <span class="n">extent</span><span class="o">=</span><span class="n">extent</span><span class="p">)</span>
                            <span class="n">batch_vols</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">postprocess_volume_batch</span><span class="p">(</span><span class="n">batch_vols</span><span class="o">=</span><span class="n">batch_vols</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># exclude symmetrized +k frequency</span>
                                                                       <span class="n">norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span>
                                                                       <span class="n">iht_downsample_scaling_correction</span><span class="o">=</span><span class="n">iht_downsample_scaling_correction</span><span class="p">,</span>
                                                                       <span class="n">lowpass_mask</span><span class="o">=</span><span class="n">lowpass_mask</span><span class="p">,</span>
                                                                       <span class="n">flip</span><span class="o">=</span><span class="n">flip</span><span class="p">,</span>
                                                                       <span class="n">invert</span><span class="o">=</span><span class="n">invert</span><span class="p">)</span>
                            <span class="n">out_mrcs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">out_dir</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">out_name</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">j</span><span class="si">:</span><span class="s1">03d</span><span class="si">}</span><span class="s1">.mrc&#39;</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">z_batch</span><span class="p">))]</span>
                            <span class="n">p</span><span class="o">.</span><span class="n">starmap</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">mrc</span><span class="o">.</span><span class="n">write</span><span class="p">,</span> <span class="n">iterable</span><span class="o">=</span><span class="nb">zip</span><span class="p">(</span><span class="n">out_mrcs</span><span class="p">,</span>
                                                                   <span class="n">batch_vols</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">out_mrcs</span><span class="p">)],</span>
                                                                   <span class="n">itertools</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_mrcs</span><span class="p">)),</span>
                                                                   <span class="n">itertools</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">angpix</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_mrcs</span><span class="p">))))</span></div>


    <span class="k">def</span> <span class="nf">_check_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                      <span class="n">out_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                      <span class="n">downsample</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check inputs are expected types, shapes, and within expected bounds for model evaluation.</span>
<span class="sd">        :param out_dir: path to output directory in which to save output mrc file(s)</span>
<span class="sd">        :param downsample: downsample reconstructed volumes to this box size (units: px) by Fourier cropping, None means to skip downsampling</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># create output directory</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">out_dir</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">out_dir</span><span class="p">)</span>

        <span class="c1"># downsample checks</span>
        <span class="k">if</span> <span class="n">downsample</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">downsample</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Boxsize must be even&quot;</span>
            <span class="k">assert</span> <span class="n">downsample</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_boxsize_ht</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Must be smaller than original box size&quot;</span>

    <span class="k">def</span> <span class="nf">_prepare_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">out_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                        <span class="n">downsample</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="n">lowpass</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="n">z</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepare inputs for repeatedly evaluating a tomoDRGN model to produce and postprocess and ensemble of volumes.</span>
<span class="sd">            generate 2d plane of coords to evaluate with appropriate extent and eventual volume scaling factor for downsampling</span>
<span class="sd">            generate lowpass filter mask</span>
<span class="sd">            calculate downsample scaling factor</span>
<span class="sd">            load z values to evaluate (if any, and write to disk if given array)</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># generate 2-D XY plane of coordinates to eventually evaluate (as cube of coords by repeating along z axis) with appropriate extent and IHT scaling factor for downsampling</span>
        <span class="k">if</span> <span class="n">downsample</span><span class="p">:</span>
            <span class="n">boxsize_ht</span> <span class="o">=</span> <span class="n">downsample</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">coords</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lat</span><span class="o">.</span><span class="n">get_downsample_coords</span><span class="p">(</span><span class="n">boxsize_new</span><span class="o">=</span><span class="n">boxsize_ht</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">extent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lat</span><span class="o">.</span><span class="n">extent</span> <span class="o">*</span> <span class="p">(</span><span class="n">downsample</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_boxsize_ht</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">iht_downsample_scaling_correction</span> <span class="o">=</span> <span class="n">downsample</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_boxsize_ht</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">3</span>
            <span class="n">angpix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_angpix</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_boxsize_ht</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">downsample</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">boxsize_ht</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_boxsize_ht</span>
            <span class="n">coords</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lat</span><span class="o">.</span><span class="n">coords</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">extent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lat</span><span class="o">.</span><span class="n">extent</span>
            <span class="n">iht_downsample_scaling_correction</span> <span class="o">=</span> <span class="mf">1.</span>
            <span class="n">angpix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_angpix</span>

        <span class="c1"># generate array to be multiplied into volumes as lowpass filter (note this takes place after removing symmetrized +k frequency row/column of pixels, and operates on np array so device None)</span>
        <span class="k">if</span> <span class="n">lowpass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lowpass_mask</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">calc_lowpass_filter_mask</span><span class="p">(</span><span class="n">boxsize</span><span class="o">=</span><span class="n">boxsize_ht</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                                                          <span class="n">angpix</span><span class="o">=</span><span class="n">angpix</span><span class="p">,</span>
                                                          <span class="n">lowpass</span><span class="o">=</span><span class="n">lowpass</span><span class="p">,</span>
                                                          <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lowpass_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">boxsize_ht</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">boxsize_ht</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">boxsize_ht</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                                   <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="c1"># unsqueeze along batch dimension</span>
        <span class="n">lowpass_mask</span> <span class="o">=</span> <span class="n">lowpass_mask</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

        <span class="c1"># prepare array of latent values to evaluate, if provided</span>
        <span class="k">if</span> <span class="n">z</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># load latent if not preloaded</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">str</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">z</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;.pkl&#39;</span><span class="p">):</span>
                    <span class="n">z</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">load_pkl</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">z</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;.txt&#39;</span><span class="p">):</span>
                    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Unrecognized file type for loading z: </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="ow">is</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
                <span class="k">pass</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Unrecognized data structure for z: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

            <span class="c1"># confirm latent array shape</span>
            <span class="k">if</span> <span class="n">z</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">z</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;zdim must be 1 or 2, got </span><span class="si">{</span><span class="n">z</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

            <span class="c1"># confirm latent dimensionality matches value loaded from config</span>
            <span class="k">assert</span> <span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">zdim</span>

            <span class="c1"># save the z values used to generate the volumes in the output directory for future convenience</span>
            <span class="n">zfile</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">out_dir</span><span class="si">}</span><span class="s1">/z_values.txt&#39;</span>
            <span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="n">zfile</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
            <span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Saved latent embeddings to decode to </span><span class="si">{</span><span class="n">zfile</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

            <span class="c1"># convert z to tensor</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">coords</span><span class="p">,</span> <span class="n">extent</span><span class="p">,</span> <span class="n">iht_downsample_scaling_correction</span><span class="p">,</span> <span class="n">angpix</span><span class="p">,</span> <span class="n">lowpass_mask</span><span class="p">,</span> <span class="n">z</span></div>



<div class="viewcode-block" id="mlp_ascii">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.mlp_ascii.html#tomodrgn.models.mlp_ascii">[docs]</a>
<span class="k">def</span> <span class="nf">mlp_ascii</span><span class="p">(</span><span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create ASCII art of a fully connected multi-layer perceptron.</span>
<span class="sd">    Constraints: number of digits in each dimension cannot exceed 5.</span>
<span class="sd">    Sample usage: ``print_mlp_ascii(input_dim=12345, hidden_dims=[128, 128, 128], output_dim=2)``</span>

<span class="sd">    :param input_dim: dimensionality of input layer</span>
<span class="sd">    :param hidden_dims: list of dimensionalities of hidden layers</span>
<span class="sd">    :param output_dim: dimensionality of output layer</span>
<span class="sd">    :return: None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">art_height</span> <span class="o">=</span> <span class="mi">9</span>  <span class="c1"># 9 lines tall</span>
    <span class="n">node_symbol</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\u25CF</span><span class="s1">&#39;</span>
    <span class="n">node_connection_symbol</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\u292C</span><span class="s1">&#39;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">art_height</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">):</span>
            <span class="c1"># draw the conneciton lines and nodes</span>
            <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">):</span>
                <span class="c1"># top or bottom line, number of nodes = len(hidden_dims)</span>
                <span class="n">line</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;        </span><span class="si">{</span><span class="n">node_symbol</span><span class="si">}</span><span class="s1"> &#39;</span> <span class="o">+</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;--- </span><span class="si">{</span><span class="n">node_symbol</span><span class="si">}</span><span class="s1"> &#39;</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span> <span class="o">+</span> <span class="s1">&#39;       &#39;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># central two lines, number of nodes = len(hidden_dims + 2)</span>
                <span class="n">line</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;  </span><span class="si">{</span><span class="n">node_symbol</span><span class="si">}</span><span class="s1"> ---&#39;</span> <span class="o">+</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39; </span><span class="si">{</span><span class="n">node_symbol</span><span class="si">}</span><span class="s1"> ---&#39;</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">))])</span> <span class="o">+</span> <span class="sa">f</span><span class="s1">&#39; </span><span class="si">{</span><span class="n">node_symbol</span><span class="si">}</span><span class="s1">  &#39;</span>
        <span class="k">elif</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">):</span>
            <span class="c1"># draw the connection lines and colons only</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># top line, number of connections with X = len(hidden_dims)-1</span>
                <span class="n">line</span> <span class="o">=</span> <span class="s1">&#39;     /     &#39;</span> <span class="o">+</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">node_connection_symbol</span><span class="si">}</span><span class="s1">     &#39;</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\\</span><span class="s1">     &#39;</span>
            <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="c1"># central two lines, number of connections with X = len(hidden_dims)-1 and need to draw colons connecting nodes</span>
                <span class="n">line</span> <span class="o">=</span> <span class="s1">&#39;  :  </span><span class="se">\\</span><span class="s1"> &#39;</span> <span class="o">+</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39; :  </span><span class="si">{</span><span class="n">node_connection_symbol</span><span class="si">}</span><span class="s1"> &#39;</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span> <span class="o">+</span> <span class="s1">&#39; :  /  :  &#39;</span>
            <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
                <span class="c1"># central two lines, number of connections with X = len(hidden_dims)-1 and need to draw colons connecting nodes</span>
                <span class="n">line</span> <span class="o">=</span> <span class="s1">&#39;  :  / &#39;</span> <span class="o">+</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39; :  </span><span class="si">{</span><span class="n">node_connection_symbol</span><span class="si">}</span><span class="s1"> &#39;</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span> <span class="o">+</span> <span class="s1">&#39; :  </span><span class="se">\\</span><span class="s1">  :  &#39;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># bottom line, number of connections with X = len(hidden_dims)-1</span>
                <span class="n">line</span> <span class="o">=</span> <span class="s1">&#39;     </span><span class="se">\\</span><span class="s1">     &#39;</span> <span class="o">+</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">node_connection_symbol</span><span class="si">}</span><span class="s1">     &#39;</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span> <span class="o">+</span> <span class="s1">&#39;/     &#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># draw the center padded node numbers</span>
            <span class="c1"># f&#39;{input_dim:^5}&#39;</span>
            <span class="n">line</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">input_dim</span><span class="si">:</span><span class="s1">^5</span><span class="si">}</span><span class="s1"> &#39;</span> <span class="o">+</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">hidden_dim</span><span class="si">:</span><span class="s1">^5</span><span class="si">}</span><span class="s1"> &#39;</span> <span class="k">for</span> <span class="n">hidden_dim</span> <span class="ow">in</span> <span class="n">hidden_dims</span><span class="p">])</span> <span class="o">+</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">output_dim</span><span class="si">:</span><span class="s1">^5</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span></div>



<div class="viewcode-block" id="print_tiltserieshetonlyvae_ascii">
<a class="viewcode-back" href="../../api/_autosummary/tomodrgn.models.print_tiltserieshetonlyvae_ascii.html#tomodrgn.models.print_tiltserieshetonlyvae_ascii">[docs]</a>
<span class="k">def</span> <span class="nf">print_tiltserieshetonlyvae_ascii</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">TiltSeriesHetOnlyVAE</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Print an ASCII art representation of a TiltSeriesHetOnlyVAE model</span>

<span class="sd">    :param model: the model to represent</span>
<span class="sd">    :return: None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">enca</span> <span class="o">=</span> <span class="n">mlp_ascii</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">in_dim</span><span class="p">,</span>
                     <span class="n">hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">hidden_dim_a</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">hidden_layers_a</span><span class="p">)],</span>
                     <span class="n">output_dim</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">out_dim_a</span><span class="p">)</span>
    <span class="n">enca</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="s2">&quot;ENCODER A - PER IMAGE&quot;</span><span class="si">:</span><span class="s1">^</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">enca</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="si">}}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">encb</span> <span class="o">=</span> <span class="n">mlp_ascii</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">out_dim_a</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">ntilts</span><span class="p">,</span>
                     <span class="n">hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">hidden_dim_b</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">hidden_layers_b</span><span class="p">)],</span>
                     <span class="n">output_dim</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">zdim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">encb</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="s2">&quot;ENCODER B - PER PARTICLE&quot;</span><span class="si">:</span><span class="s1">^</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">encb</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="si">}}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">dec</span> <span class="o">=</span> <span class="n">mlp_ascii</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">in_dim</span><span class="p">,</span>
                    <span class="n">hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">hidden_layers</span><span class="p">)],</span>
                    <span class="n">output_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">dec</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="s2">&quot;DECODER - PER VOXEL&quot;</span><span class="si">:</span><span class="s1">^</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dec</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="si">}}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">enca_line</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">encb_line</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">dec_line</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">enca_line</span><span class="p">,</span> <span class="n">encb_line</span><span class="p">,</span> <span class="n">dec_line</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">enca</span><span class="p">,</span> <span class="n">encb</span><span class="p">,</span> <span class="n">dec</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">output</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span></div>

</pre></div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024, Barrett M Powell, Joseph H Davis.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.0.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>